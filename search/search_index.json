{
    "docs": [
        {
            "location": "/",
            "text": "o2r web API documentation\n\u00b6\n\n\n\n\nCurrent version of the API\n: \nv1\n\n\nAbout\n\u00b6\n\n\nThe o2r web API acts as the interface between the \no2r\n \nmicroservices\n and the \nweb interface\n.\n\n\nThe API provides services around the executable research compendium (ERC), or \"compendium\" for short, which is documented \nin the ERC spec\n.\n\n\nGeneral notes\n\u00b6\n\n\nThe API is implemented as a \nREST\nful API. The entrypoint for the current version is \n/api/v1\n.\n\n\nUnless specified otherwise, responses are always in JSON format.\nBody parameters in \nPOST\n requests are expected in \nmultipart/form-data\n format.\nRequests to the API should always be made with a secure connection using \nHTTPS\n.\nSome requests require \nauthentication\n with a specific \nuser level\n.\n\n\nWe also provide a \nsimple Postman collection\n (\ngetpostman.com\n), so that you can comfortably explore the API.\n\n\nLicense\n\u00b6\n\n\n\n\nThe o2r Executable Research Compendium specification is licensed under \nCreative Commons CC0 1.0 Universal License\n, see file \nLICENSE\n.\nTo the extent possible under law, the people who associated CC0 with this work have waived all copyright and related or neighboring rights to this work.\nThis work is published from: Germany.",
            "title": "Home"
        },
        {
            "location": "/#o2r-web-api-documentation",
            "text": "Current version of the API :  v1",
            "title": "o2r web API documentation"
        },
        {
            "location": "/#about",
            "text": "The o2r web API acts as the interface between the  o2r   microservices  and the  web interface .  The API provides services around the executable research compendium (ERC), or \"compendium\" for short, which is documented  in the ERC spec .",
            "title": "About"
        },
        {
            "location": "/#general-notes",
            "text": "The API is implemented as a  REST ful API. The entrypoint for the current version is  /api/v1 .  Unless specified otherwise, responses are always in JSON format.\nBody parameters in  POST  requests are expected in  multipart/form-data  format.\nRequests to the API should always be made with a secure connection using  HTTPS .\nSome requests require  authentication  with a specific  user level .  We also provide a  simple Postman collection  ( getpostman.com ), so that you can comfortably explore the API.",
            "title": "General notes"
        },
        {
            "location": "/#license",
            "text": "The o2r Executable Research Compendium specification is licensed under  Creative Commons CC0 1.0 Universal License , see file  LICENSE .\nTo the extent possible under law, the people who associated CC0 with this work have waived all copyright and related or neighboring rights to this work.\nThis work is published from: Germany.",
            "title": "License"
        },
        {
            "location": "/compendium/upload/",
            "text": "Upload via API\n\u00b6\n\n\nUpload a research workspace or full compendium as a compressed \n.zip\n archive with an HTTP \nPOST\n request using \nmultipart/form-data\n.\n\n\nThe upload is only allowed for logged in users.\nUpon successful extraction of archive and processing of the contents, the \nid\n for the new compendium is returned.\n\n\n\n\nRequired user level and authentication\n\n\nThe user creating a new compendium must have the required \nuser level\n.\nRequests must be authenticated with a cookie \nconnect.sid\n, see \nuser authentication\n.\n\n\n\n\ncurl -F \"compendium=@compendium.zip;type=application/zip\" \\\n    -F content_type=compendium \\\n    --cookie \"connect.sid=<cookie string here>\" \\\n     https://\u2026/api/v1/compendium \n\n\n\n\ncurl -F \"compendium=@path/to/workspace.zip;type=application/zip\" \\\n    -F content_type=workspace \\\n    --cookie \"connect.sid=<cookie string here>\" \\\n     https://\u2026/api/v1/compendium \n\n\n\n\n200 OK\n\n{\"id\":\"a4Ndl\"}\n\n\n\n\n\n\nImportant\n\n\nAfter successful upload the \ncandidate process\n must be completed for workspaces.\n\n\n\n\nBody parameters for compendium upload\n\u00b6\n\n\n\n\ncompendium\n - The archive file\n\n\ncontent_type\n - Form of archive. One of the following:\n\n\ncompendium\n - compendium, which is expected to be complete and valid, for \nexamination\n of a compendium\n\n\nworkspace\n - formless workspace, for \ncreation\n of a compendium\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\nIf a complete ERC is submitted as a workspace, it may result in an error, or the contained metadata and other files may be overwritten by the creation process.\n\n\n\n\nError responses for compendium upload\n\u00b6\n\n\n400 Bad Request\n\n{\"error\":\"provided content_type not implemented\"}\n\n\n\n\n401 Unauthorized\n\n{\"error\":\"user is not authenticated\"}\n\n\n\n\n401 Unauthorized\n\n{\"error\":\"user level does not allow compendium creation\"}\n\n\n\n\n422 Unprocessable Entity\n\n\n\n\nFor local testing you can quickly upload some of the example compendia and workspaces from the \nerc-examples\n project.",
            "title": "API upload"
        },
        {
            "location": "/compendium/upload/#upload-via-api",
            "text": "Upload a research workspace or full compendium as a compressed  .zip  archive with an HTTP  POST  request using  multipart/form-data .  The upload is only allowed for logged in users.\nUpon successful extraction of archive and processing of the contents, the  id  for the new compendium is returned.   Required user level and authentication  The user creating a new compendium must have the required  user level .\nRequests must be authenticated with a cookie  connect.sid , see  user authentication .   curl -F \"compendium=@compendium.zip;type=application/zip\" \\\n    -F content_type=compendium \\\n    --cookie \"connect.sid=<cookie string here>\" \\\n     https://\u2026/api/v1/compendium   curl -F \"compendium=@path/to/workspace.zip;type=application/zip\" \\\n    -F content_type=workspace \\\n    --cookie \"connect.sid=<cookie string here>\" \\\n     https://\u2026/api/v1/compendium   200 OK\n\n{\"id\":\"a4Ndl\"}   Important  After successful upload the  candidate process  must be completed for workspaces.",
            "title": "Upload via API"
        },
        {
            "location": "/compendium/upload/#body-parameters-for-compendium-upload",
            "text": "compendium  - The archive file  content_type  - Form of archive. One of the following:  compendium  - compendium, which is expected to be complete and valid, for  examination  of a compendium  workspace  - formless workspace, for  creation  of a compendium      Warning  If a complete ERC is submitted as a workspace, it may result in an error, or the contained metadata and other files may be overwritten by the creation process.",
            "title": "Body parameters for compendium upload"
        },
        {
            "location": "/compendium/upload/#error-responses-for-compendium-upload",
            "text": "400 Bad Request\n\n{\"error\":\"provided content_type not implemented\"}  401 Unauthorized\n\n{\"error\":\"user is not authenticated\"}  401 Unauthorized\n\n{\"error\":\"user level does not allow compendium creation\"}  422 Unprocessable Entity  For local testing you can quickly upload some of the example compendia and workspaces from the  erc-examples  project.",
            "title": "Error responses for compendium upload"
        },
        {
            "location": "/compendium/public_share/",
            "text": "Public share\n\u00b6\n\n\nLoad a research compendium by submitting a link to a cloud resource using an HTTP \nPOST\n request using \nmultipart/form-data\n.\n\n\nCurrently, the following repositories are supported:\n\n\n\n\nSciebo\n\n\nZenodo\n\n\nZenodo Sandbox\n\n\n\n\nCommon\n\u00b6\n\n\nAll repositories use the same API endpoint \nhttps://\u2026/api/v1/compendium\n, but with different required/optional parameters.\n\n\nThe upload is only allowed for logged in users.\n\n\n\n\nRequired user level and authentication\n\n\nThe user creating a new compendium must have the required \nuser level\n.\nRequests must be authenticated with a cookie \nconnect.sid\n, see \nuser authentication\n.\n\n\n\n\nTo run the load from the command line, login on the website and open you browser cookies.\nFind a cookie issued by \no2r.uni-muenster.de\n with the name \nconnect.sid\n.\nCopy the contents of the cookie into the request example below.\n\n\nUpon successful download from the public share, the \nid\n for the new compendium is returned.\n\n\ncurl -F \"content_type=compendium\" \\\n    -F \"share_url=https://uni-muenster.sciebo.de/index.php/s/G8vxQ1h50V4HpuA\"  \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium\n\n\n\n\n200 OK\n\n{\"id\":\"b9Faz\"}\n\n\n\n\n\n\nImportant\n\n\nAfter successful load from a public share, the \ncandidate process\n applies.\n\n\n\n\nSciebo\n\u00b6\n\n\nSciebo\n is a cloud storage service at North Rhine-Westphalian universities.\nAlthough it builds on ownCloud and the implementation might be able to handle any ownCloud link, only Sciebo's publish shares are supported by this API. \n\n\nFile selection\n\u00b6\n\n\nDepending on the public share contents different processes are triggered:\n\n\n\n\nIf a file named \nbagit.txt\n is found, the directory is checked for Bagit validity\n\n\nIf a single zip file is found, the file is extracted, if multiple zip files are found, the filename has to be specified, otherwise an error is returned\n\n\nIf a single subdirectory is found, the loader uses that subdirectory as the base directory for loading\n\n\nDepending on the value of \ncontent_type\n (see below), the public share contents are treated as a complete compendium or as a  workspace\n\n\n\n\nBody parameters for creating compendium from public share\n\u00b6\n\n\n\n\nshare_url\n - The Sciebo link to the public share (required)\n\n\ncontent_type\n - Form of archive. One of the following (required):\n\n\ncompendium\n - complete compendium\n\n\nworkspace\n - formless workspace\n\n\n\n\n\n\npath\n - Path to a subdirectory or a zip file in the public share (optional)\n\n\ndefault is \n/\n\n\nthe leading \n/\n is optional, the loader supports both ways\n\n\nwhen a directory has multiple zip files, the path can be used to specify which file is used, e.g. \npath=/metatainer.zip\n\n\n\n\n\n\n\n\nExamples\n\u00b6\n\n\ncurl -F \"content_type=compendium\" \\\n    -F \"share_url=https://uni-muenster.sciebo.de/index.php/s/G8vxQ1h50V4HpuA\"  \\\n    -F \"path=/metatainer\" \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium\n\n\n\n\nError responses for creating compendium from public share\n\u00b6\n\n\n401 Unauthorized\n\n{\"error\":\"unauthorized: user level does not allow compendium creation\"}\n\n\n\n\n403 Forbidden\n\n{\"error\":\"public share host is not allowed\"}\n\n\n\n\n422 Unprocessable Entity\n\n{\"error\":\"files with unsupported encoding detected: [{'file':'/tmp/o2r/compendium/ejpmi/data/test.txt','encoding':'Shift_JIS'}]\"}\n\n\n\n\nExample data\n\u00b6\n\n\nFor testing purposes you can use the following public share, which contains a few ready-to-use compendia:\n\n\n`https://uni-muenster.sciebo.de/s/G8vxQ1h50V4HpuA\n\n\nZenodo\n\u00b6\n\n\nBody parameters for creating a compendium from a Zenodo record\n\u00b6\n\n\n\n\nIdentification of the Zenodo record, one of the folloing is required:\n\n\nshare_url\n - The link to the zenodo record (optional). May be a link to https://zenodo.org or https://doi.org\n\n\ndoi\n - A \nDOI\n resolving to the zenodo record (optional)\n\n\nzenodo_record_id\n - The ID of the zenodo record (optional)\n\n\n\n\n\n\ncontent_type\n - Form of archive. One of the following (required):\n\n\ncompendium\n - complete compendium for \ninspection\n\n\nworkspace\n - formless workspace for \ncreation\n\n\n\n\n\n\nfilename\n - Filename of your compendium. For now, only zip-files are supported. (optional)\n\n\nif no \nfilename\n is provided the first zip file is selected\n\n\nmultiple files are currently not supported\n\n\n\n\n\n\n\n\nThere must at least one url parameter that resolves to a zenodo record. I.e. one of the following:\n\n\nExamples\n\u00b6\n\n\n\n\nZenodo Record URL (with optional filename parameter)\n\n\n\n\ncurl -F \"content_type=compendium\" \\\n    -F \"zenodo_url=https://sandbox.zenodo.org/record/69114\"  \\\n    -F \"filename=metatainer.zip\" \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium\n\n\n\n\n\n\nDOI\n\n\n\n\ncurl -F \"content_type=compendium\" \\\n    -F \"doi=10.5072/zenodo.69114\"  \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium\n\n\n\n\n\n\nZenodo Record ID\n\n\n\n\ncurl -F \"content_type=compendium\" \\\n    -F \"zenodo_record_id=69114\"  \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium\n\n\n\n\nIf the Zenodo record id is supplied through the \ndoi\n or \nzenodo_record_id\n parameter, or if the \nshare_url\n parameter is a \ndoi.org\n URL, a default base URL for the file download is used as selected by the API maintainer. This may be:\n\n\n\n\nhttps://zenodo.org\n or\n\n\nhttps://sandbox.zenodo.org\n\n\n\n\nError responses for creating compendium from a Zenodo record\n\u00b6\n\n\n401 Unauthorized\n\n{\"error\":\"unauthorized: user level does not allow compendium creation\"}\n\n\n\n\n403 Forbidden\n\n{\"error\":\"host is not allowed\"}\n\n\n\n\n422 Unprocessable Entity\n\n{\"error\":\"public share URL is invalid\"}\n\n\n\n\n422 Unprocessable Entity\n\n{\"error\":\"DOI is invalid\"}\n\n\n\n\n422 Unprocessable Entity\n\n{\"error\":\"files with unsupported encoding detected: [{'file':'/tmp/o2r/compendium/ejpmi/data/test.txt','encoding':'Shift_JIS'}]\"}\n\n\n\n\nExample data\n\u00b6\n\n\nFor testing purposes you can use the following public shares.\nThey contain the a compendium with metadata.\n\n\n\n\nSciebo: \nhttps://uni-muenster.sciebo.de/index.php/s/G8vxQ1h50V4HpuA\n\n\nZenodo: \nhttps://sandbox.zenodo.org/record/69114",
            "title": "Public share submission"
        },
        {
            "location": "/compendium/public_share/#public-share",
            "text": "Load a research compendium by submitting a link to a cloud resource using an HTTP  POST  request using  multipart/form-data .  Currently, the following repositories are supported:   Sciebo  Zenodo  Zenodo Sandbox",
            "title": "Public share"
        },
        {
            "location": "/compendium/public_share/#common",
            "text": "All repositories use the same API endpoint  https://\u2026/api/v1/compendium , but with different required/optional parameters.  The upload is only allowed for logged in users.   Required user level and authentication  The user creating a new compendium must have the required  user level .\nRequests must be authenticated with a cookie  connect.sid , see  user authentication .   To run the load from the command line, login on the website and open you browser cookies.\nFind a cookie issued by  o2r.uni-muenster.de  with the name  connect.sid .\nCopy the contents of the cookie into the request example below.  Upon successful download from the public share, the  id  for the new compendium is returned.  curl -F \"content_type=compendium\" \\\n    -F \"share_url=https://uni-muenster.sciebo.de/index.php/s/G8vxQ1h50V4HpuA\"  \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium  200 OK\n\n{\"id\":\"b9Faz\"}   Important  After successful load from a public share, the  candidate process  applies.",
            "title": "Common"
        },
        {
            "location": "/compendium/public_share/#sciebo",
            "text": "Sciebo  is a cloud storage service at North Rhine-Westphalian universities.\nAlthough it builds on ownCloud and the implementation might be able to handle any ownCloud link, only Sciebo's publish shares are supported by this API.",
            "title": "Sciebo"
        },
        {
            "location": "/compendium/public_share/#file-selection",
            "text": "Depending on the public share contents different processes are triggered:   If a file named  bagit.txt  is found, the directory is checked for Bagit validity  If a single zip file is found, the file is extracted, if multiple zip files are found, the filename has to be specified, otherwise an error is returned  If a single subdirectory is found, the loader uses that subdirectory as the base directory for loading  Depending on the value of  content_type  (see below), the public share contents are treated as a complete compendium or as a  workspace",
            "title": "File selection"
        },
        {
            "location": "/compendium/public_share/#body-parameters-for-creating-compendium-from-public-share",
            "text": "share_url  - The Sciebo link to the public share (required)  content_type  - Form of archive. One of the following (required):  compendium  - complete compendium  workspace  - formless workspace    path  - Path to a subdirectory or a zip file in the public share (optional)  default is  /  the leading  /  is optional, the loader supports both ways  when a directory has multiple zip files, the path can be used to specify which file is used, e.g.  path=/metatainer.zip",
            "title": "Body parameters for creating compendium from public share"
        },
        {
            "location": "/compendium/public_share/#examples",
            "text": "curl -F \"content_type=compendium\" \\\n    -F \"share_url=https://uni-muenster.sciebo.de/index.php/s/G8vxQ1h50V4HpuA\"  \\\n    -F \"path=/metatainer\" \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium",
            "title": "Examples"
        },
        {
            "location": "/compendium/public_share/#error-responses-for-creating-compendium-from-public-share",
            "text": "401 Unauthorized\n\n{\"error\":\"unauthorized: user level does not allow compendium creation\"}  403 Forbidden\n\n{\"error\":\"public share host is not allowed\"}  422 Unprocessable Entity\n\n{\"error\":\"files with unsupported encoding detected: [{'file':'/tmp/o2r/compendium/ejpmi/data/test.txt','encoding':'Shift_JIS'}]\"}",
            "title": "Error responses for creating compendium from public share"
        },
        {
            "location": "/compendium/public_share/#example-data",
            "text": "For testing purposes you can use the following public share, which contains a few ready-to-use compendia:  `https://uni-muenster.sciebo.de/s/G8vxQ1h50V4HpuA",
            "title": "Example data"
        },
        {
            "location": "/compendium/public_share/#zenodo",
            "text": "",
            "title": "Zenodo"
        },
        {
            "location": "/compendium/public_share/#body-parameters-for-creating-a-compendium-from-a-zenodo-record",
            "text": "Identification of the Zenodo record, one of the folloing is required:  share_url  - The link to the zenodo record (optional). May be a link to https://zenodo.org or https://doi.org  doi  - A  DOI  resolving to the zenodo record (optional)  zenodo_record_id  - The ID of the zenodo record (optional)    content_type  - Form of archive. One of the following (required):  compendium  - complete compendium for  inspection  workspace  - formless workspace for  creation    filename  - Filename of your compendium. For now, only zip-files are supported. (optional)  if no  filename  is provided the first zip file is selected  multiple files are currently not supported     There must at least one url parameter that resolves to a zenodo record. I.e. one of the following:",
            "title": "Body parameters for creating a compendium from a Zenodo record"
        },
        {
            "location": "/compendium/public_share/#examples_1",
            "text": "Zenodo Record URL (with optional filename parameter)   curl -F \"content_type=compendium\" \\\n    -F \"zenodo_url=https://sandbox.zenodo.org/record/69114\"  \\\n    -F \"filename=metatainer.zip\" \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium   DOI   curl -F \"content_type=compendium\" \\\n    -F \"doi=10.5072/zenodo.69114\"  \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium   Zenodo Record ID   curl -F \"content_type=compendium\" \\\n    -F \"zenodo_record_id=69114\"  \\\n    --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/compendium  If the Zenodo record id is supplied through the  doi  or  zenodo_record_id  parameter, or if the  share_url  parameter is a  doi.org  URL, a default base URL for the file download is used as selected by the API maintainer. This may be:   https://zenodo.org  or  https://sandbox.zenodo.org",
            "title": "Examples"
        },
        {
            "location": "/compendium/public_share/#error-responses-for-creating-compendium-from-a-zenodo-record",
            "text": "401 Unauthorized\n\n{\"error\":\"unauthorized: user level does not allow compendium creation\"}  403 Forbidden\n\n{\"error\":\"host is not allowed\"}  422 Unprocessable Entity\n\n{\"error\":\"public share URL is invalid\"}  422 Unprocessable Entity\n\n{\"error\":\"DOI is invalid\"}  422 Unprocessable Entity\n\n{\"error\":\"files with unsupported encoding detected: [{'file':'/tmp/o2r/compendium/ejpmi/data/test.txt','encoding':'Shift_JIS'}]\"}",
            "title": "Error responses for creating compendium from a Zenodo record"
        },
        {
            "location": "/compendium/public_share/#example-data_1",
            "text": "For testing purposes you can use the following public shares.\nThey contain the a compendium with metadata.   Sciebo:  https://uni-muenster.sciebo.de/index.php/s/G8vxQ1h50V4HpuA  Zenodo:  https://sandbox.zenodo.org/record/69114",
            "title": "Example data"
        },
        {
            "location": "/compendium/candidate/",
            "text": "Candidate process\n\u00b6\n\n\nAfter uploading a compendium is \nnot\n instantly publicly available.\nIt is merely a \ncandidate\n, because metadata still must be completed for the compendium to be valid.\n\n\nThe following process models this intermediate state of a compendium.\n\n\nCreation and view\n\u00b6\n\n\nCandidates can be identified by the property \ncandidate\n.\nIt is set to \ntrue\n after creating a new compendium by \nupload\n or \npublic share submission\n \nand\n the authoring user having reviewed the metadata.\n\n\n\n\nNote\n\n\nIt is not possible to circumvent the metadata review.\nOnly a successful \nmetadata update\n can set \ncandidate: true\n.\n\n\n\n\nExample:\n\n\n{\n  \"id\":\"12345\",\n  \"metadata\": \u2026 ,\n  \"created\": \"2016-08-01T13:57:40.760Z\",\n  \"files\": \u2026,\n  \"candidate\": true\n}\n\n\n\n\nOnly the creating user and users with \nrequired level\n can view a candidate and see the \ncandidate\n property while it is \ntrue\n.\n\n\nWhen accessing a \nlist of compendia\n for a specific user \nas that user\n, then this list is extended by available candidates.\nThe candidates may be added to the response independently from any pagination settings, i.e. if a client requests the first 10 compendia for a user having two candidates, the client should be prepared to handle 12 items in the response.\n\n\nMetadata review and saving\n\u00b6\n\n\nAfter the user has reviewed and potentially updated the metadata as required and \nsaved them\n successfully, then the candidate status is changed (\ncandidate: false\n) and the compendium is publicly available.\n\n\nThe \ncandidate\n property is not exposed any more if it is \nfalse\n.\n\n\nIt is \nnot\n possible to save invalid metadata or to manually change the \ncandidate\n property, therefore a compendium cannot become a candidate again after successful completion of the creation.\n\n\nDeletion\n\u00b6\n\n\nUnlike published compendia, a candidate can be deleted by a the authoring user, see \ndelete compendium\n.",
            "title": "Candidates"
        },
        {
            "location": "/compendium/candidate/#candidate-process",
            "text": "After uploading a compendium is  not  instantly publicly available.\nIt is merely a  candidate , because metadata still must be completed for the compendium to be valid.  The following process models this intermediate state of a compendium.",
            "title": "Candidate process"
        },
        {
            "location": "/compendium/candidate/#creation-and-view",
            "text": "Candidates can be identified by the property  candidate .\nIt is set to  true  after creating a new compendium by  upload  or  public share submission   and  the authoring user having reviewed the metadata.   Note  It is not possible to circumvent the metadata review.\nOnly a successful  metadata update  can set  candidate: true .   Example:  {\n  \"id\":\"12345\",\n  \"metadata\": \u2026 ,\n  \"created\": \"2016-08-01T13:57:40.760Z\",\n  \"files\": \u2026,\n  \"candidate\": true\n}  Only the creating user and users with  required level  can view a candidate and see the  candidate  property while it is  true .  When accessing a  list of compendia  for a specific user  as that user , then this list is extended by available candidates.\nThe candidates may be added to the response independently from any pagination settings, i.e. if a client requests the first 10 compendia for a user having two candidates, the client should be prepared to handle 12 items in the response.",
            "title": "Creation and view"
        },
        {
            "location": "/compendium/candidate/#metadata-review-and-saving",
            "text": "After the user has reviewed and potentially updated the metadata as required and  saved them  successfully, then the candidate status is changed ( candidate: false ) and the compendium is publicly available.  The  candidate  property is not exposed any more if it is  false .  It is  not  possible to save invalid metadata or to manually change the  candidate  property, therefore a compendium cannot become a candidate again after successful completion of the creation.",
            "title": "Metadata review and saving"
        },
        {
            "location": "/compendium/candidate/#deletion",
            "text": "Unlike published compendia, a candidate can be deleted by a the authoring user, see  delete compendium .",
            "title": "Deletion"
        },
        {
            "location": "/compendium/view/",
            "text": "View compendium\n\u00b6\n\n\nList compendia\n\u00b6\n\n\nReturns up to 100 results by default.\n\n\ncurl https://\u2026/api/v1/compendium?limit=100&start=2\n\n\nGET /api/v1/compendium?limit=100&start=2\n\n\n200 OK\n\n{\n  \"results\": [\n    \"nkm4b\",\n    \"asdis\",\n    \"nb2sm\",\n    \u2026\n  ]\n}\n\n\n\n\nYou can also filter the results.\n\n\n\n\nFilter by \nuser\n:\n\n\ncurl https://\u2026/api/v1/compendium?user=0000-0002-1825-0097\n\n\nGET /api/v1/compendium?user=0000-0002-1825-0097\n\n\n\n\n\n\nFilter by \ndoi\n:\n\n\ncurl https://\u2026/api/v1/compendium?doi=10.9999%2Ftest\n\n\nGET /api/v1/compendium?doi=10.9999%2Ftest\n\n\n\n\n\n\n\n\n200 OK\n\n{\n  \"results\": [\n    \"nkm4b\",\n    \"nb2sm\"\n  ]\n}\n\n\n\n\nIf there is no compendium found, the service returns an empty list.\n\n\nGET /api/v1/compendium?doi=not_a_doi\n\n\n200 OK\n\n{\n  \"results\": []\n}\n\n\n\n\nURL parameters for compendium lists\n\u00b6\n\n\n\n\njob_id\n - Comma-separated list of related job ids to filter by.\n\n\nuser\n - Public user identifier to filter by.\n\n\ndoi\n - A \nDOI\n to filter by.\n\n\nstart\n - Starting point of the result list. \nstart - 1\n results are skipped. Defaults to \n1\n.\n\n\nlimit\n - Limits the number of results in the response. Defaults to \n100\n.\n\n\n\n\nView single compendium\n\u00b6\n\n\nThis includes the complete metadata set, related job ids and a tree representation of the included \nfiles\n. The \ncreated\n timestamp refers to the upload of the compendium. It is formated as ISO8601.\n\n\ncurl https://\u2026/api/v1/$ID\n\n\nGET /api/v1/compendium/:id\n\n\n200 OK\n\n{\n  \"id\":\"comid\",\n  \"metadata\": \u2026 ,\n  \"created\": \"2016-08-01T13:57:40.760Z\",\n  \"files\": \u2026\n }\n\n\n\n\nURL parameters for single compendium view\n\u00b6\n\n\n\n\n:id\n - the compendiums id\n\n\n\n\nError responses for single compendium view\n\u00b6\n\n\n404 Not Found\n\n{\"error\":\"no compendium with this id\"}\n\n\n\n\nList related execution jobs\n\u00b6\n\n\ncurl https://\u2026/api/v1/compendium/$ID/jobs\n\n\nGET /api/v1/compendium/:id/jobs\n\n\n200 OK\n{\n  \"results\": [\n    \"nkm4L\",\n    \"asdi5\",\n    \"nb2sg\",\n    \u2026\n  ]\n}\n\n\n\n\nIf a compendium does not have any jobs yet, the returned list is empty.\n\n\n200 OK\n{\n  \"results\": [ ]\n}\n\n\n\n\nURL parameters for related execution jobs\n\u00b6\n\n\n\n\n:id\n - compendium id that the results should be related to",
            "title": "View"
        },
        {
            "location": "/compendium/view/#view-compendium",
            "text": "",
            "title": "View compendium"
        },
        {
            "location": "/compendium/view/#list-compendia",
            "text": "Returns up to 100 results by default.  curl https://\u2026/api/v1/compendium?limit=100&start=2  GET /api/v1/compendium?limit=100&start=2  200 OK\n\n{\n  \"results\": [\n    \"nkm4b\",\n    \"asdis\",\n    \"nb2sm\",\n    \u2026\n  ]\n}  You can also filter the results.   Filter by  user :  curl https://\u2026/api/v1/compendium?user=0000-0002-1825-0097  GET /api/v1/compendium?user=0000-0002-1825-0097    Filter by  doi :  curl https://\u2026/api/v1/compendium?doi=10.9999%2Ftest  GET /api/v1/compendium?doi=10.9999%2Ftest     200 OK\n\n{\n  \"results\": [\n    \"nkm4b\",\n    \"nb2sm\"\n  ]\n}  If there is no compendium found, the service returns an empty list.  GET /api/v1/compendium?doi=not_a_doi  200 OK\n\n{\n  \"results\": []\n}",
            "title": "List compendia"
        },
        {
            "location": "/compendium/view/#url-parameters-for-compendium-lists",
            "text": "job_id  - Comma-separated list of related job ids to filter by.  user  - Public user identifier to filter by.  doi  - A  DOI  to filter by.  start  - Starting point of the result list.  start - 1  results are skipped. Defaults to  1 .  limit  - Limits the number of results in the response. Defaults to  100 .",
            "title": "URL parameters for compendium lists"
        },
        {
            "location": "/compendium/view/#view-single-compendium",
            "text": "This includes the complete metadata set, related job ids and a tree representation of the included  files . The  created  timestamp refers to the upload of the compendium. It is formated as ISO8601.  curl https://\u2026/api/v1/$ID  GET /api/v1/compendium/:id  200 OK\n\n{\n  \"id\":\"comid\",\n  \"metadata\": \u2026 ,\n  \"created\": \"2016-08-01T13:57:40.760Z\",\n  \"files\": \u2026\n }",
            "title": "View single compendium"
        },
        {
            "location": "/compendium/view/#url-parameters-for-single-compendium-view",
            "text": ":id  - the compendiums id",
            "title": "URL parameters for single compendium view"
        },
        {
            "location": "/compendium/view/#error-responses-for-single-compendium-view",
            "text": "404 Not Found\n\n{\"error\":\"no compendium with this id\"}",
            "title": "Error responses for single compendium view"
        },
        {
            "location": "/compendium/view/#list-related-execution-jobs",
            "text": "curl https://\u2026/api/v1/compendium/$ID/jobs  GET /api/v1/compendium/:id/jobs  200 OK\n{\n  \"results\": [\n    \"nkm4L\",\n    \"asdi5\",\n    \"nb2sg\",\n    \u2026\n  ]\n}  If a compendium does not have any jobs yet, the returned list is empty.  200 OK\n{\n  \"results\": [ ]\n}",
            "title": "List related execution jobs"
        },
        {
            "location": "/compendium/view/#url-parameters-for-related-execution-jobs",
            "text": ":id  - compendium id that the results should be related to",
            "title": "URL parameters for related execution jobs"
        },
        {
            "location": "/compendium/delete/",
            "text": "Delete compendium\n\u00b6\n\n\nTo delete a compendium \ncandidate\n, an HTTP \nDELETE\n request can be send to the compendium endpoint.\n\n\n\n\nImportant\n\n\nOnce a compendium is not a candidate anymore, it \ncannot\n be deleted via the API.\n\n\n\n\n\n\nRequired user level\n\n\nThe user deleting a candidate must be the author or have the required \nuser level\n.\n\n\n\n\nRequest\n\u00b6\n\n\nThe following request deletes the compendium with the identifier \n12345\n, including metadata and files.\n\n\ncurl -X DELETE https://\u2026/api/v1/compendium/12345 \\\n    --cookie \"connect.sid=<code string here>\"\n\n\n\n\nResponse\n\u00b6\n\n\nThe response has an HTTP status of \n204\n and an empty body for successful deletion.\n\n\n204 OK\n\n\n\n\nError responses for compendium delete\n\u00b6\n\n\n401 Unauthorized\n\n{\"error\":\"not authorized\"}\n\n\n\n\n403 Forbidden\n\n{\"error\":\"user level not sufficient to delete compendium\"}\n\n\n\n\n404 Not Found\n\n{\"error\":\"compendium not found\"}",
            "title": "Delete"
        },
        {
            "location": "/compendium/delete/#delete-compendium",
            "text": "To delete a compendium  candidate , an HTTP  DELETE  request can be send to the compendium endpoint.   Important  Once a compendium is not a candidate anymore, it  cannot  be deleted via the API.    Required user level  The user deleting a candidate must be the author or have the required  user level .",
            "title": "Delete compendium"
        },
        {
            "location": "/compendium/delete/#request",
            "text": "The following request deletes the compendium with the identifier  12345 , including metadata and files.  curl -X DELETE https://\u2026/api/v1/compendium/12345 \\\n    --cookie \"connect.sid=<code string here>\"",
            "title": "Request"
        },
        {
            "location": "/compendium/delete/#response",
            "text": "The response has an HTTP status of  204  and an empty body for successful deletion.  204 OK",
            "title": "Response"
        },
        {
            "location": "/compendium/delete/#error-responses-for-compendium-delete",
            "text": "401 Unauthorized\n\n{\"error\":\"not authorized\"}  403 Forbidden\n\n{\"error\":\"user level not sufficient to delete compendium\"}  404 Not Found\n\n{\"error\":\"compendium not found\"}",
            "title": "Error responses for compendium delete"
        },
        {
            "location": "/compendium/download/",
            "text": "Download compendium\n\u00b6\n\n\nDownload compendium files as an archive.\n\n\n\n\nWarning\n\n\nThis download feature does \nnot\n provide access to complete and valid compendia, because it does not comprise an update of the \npackaging\n, while it does include \nbrokered metadata files\n.\nTo download a valid compendium, create a \nshipment\n with the appropriate recipient.\n\n\n\n\nSupported formats are as follows:\n\n\n\n\nzip\n\n\ntar\n\n\ntar.gz\n\n\n\n\nRequests\n\u00b6\n\n\nGET /api/v1/compendium/$ID.zip\n\n\nGET /api/v1/compendium/:id.zip\nGET /api/v1/compendium/:id.tar\nGET /api/v1/compendium/:id.tar.gz\nGET /api/v1/compendium/:id.tar?gzip\nGET /api/v1/compendium/:id.zip?image=false\n\n\n\n\nURL parameters for compendium download\n\u00b6\n\n\n\n\n:id\n - the compendiums id\n\n\n?gzip\n - \nonly for .tar endpoint\n - compress tarball with gzip\n\n\n?image=true\n or \n?image=false\n - include tarball of Docker image in the archive, default is \ntrue\n\n\n\n\nResponse\n\u00b6\n\n\nThe response is a file attachment. The suggested file name is available in the HTTP header \ncontent-disposition\n using the respective file extension for a file named with the compendium identifier (e.g. \nwdpV9.zip\n, \nUh1o0.tar\n, or \nLBIt1.tar.gz\n).\n\n\nThe \ncontent-type\n header also reflects the respective format, which can take the following values:\n\n\n\n\napplication/zip\n for ZIP archive\n\n\napplication/x-tar\n for TAR archive\n\n\napplication/octet-stream\n for gzipped TAR\n\n\n\n\n200 OK\nContent-Type: application/zip\nTransfer-Encoding: chunked\nContent-Disposition: attachment; filename=\"$ID.zip\"\nX-Response-Time: 13.556ms\n\n\n\n\nThe zip file contains a comment with the original URL.\n\n\n$ unzip -z CXE1c.zip\nArchive:  CXE1c.zip\nCreated by o2r [https://\u2026/api/v1/compendium/CXE1c.zip]\n\n\n\n\nError responses for compendium download\n\u00b6\n\n\n404 Not Found\n\n{\"error\":\"no compendium with this id\"}\n\n\n\n\n400 Bad Request\n\n{\"error\":\"no job found for this compendium, run a job before downloading with image\"}",
            "title": "Download"
        },
        {
            "location": "/compendium/download/#download-compendium",
            "text": "Download compendium files as an archive.   Warning  This download feature does  not  provide access to complete and valid compendia, because it does not comprise an update of the  packaging , while it does include  brokered metadata files .\nTo download a valid compendium, create a  shipment  with the appropriate recipient.   Supported formats are as follows:   zip  tar  tar.gz",
            "title": "Download compendium"
        },
        {
            "location": "/compendium/download/#requests",
            "text": "GET /api/v1/compendium/$ID.zip  GET /api/v1/compendium/:id.zip\nGET /api/v1/compendium/:id.tar\nGET /api/v1/compendium/:id.tar.gz\nGET /api/v1/compendium/:id.tar?gzip\nGET /api/v1/compendium/:id.zip?image=false",
            "title": "Requests"
        },
        {
            "location": "/compendium/download/#url-parameters-for-compendium-download",
            "text": ":id  - the compendiums id  ?gzip  -  only for .tar endpoint  - compress tarball with gzip  ?image=true  or  ?image=false  - include tarball of Docker image in the archive, default is  true",
            "title": "URL parameters for compendium download"
        },
        {
            "location": "/compendium/download/#response",
            "text": "The response is a file attachment. The suggested file name is available in the HTTP header  content-disposition  using the respective file extension for a file named with the compendium identifier (e.g.  wdpV9.zip ,  Uh1o0.tar , or  LBIt1.tar.gz ).  The  content-type  header also reflects the respective format, which can take the following values:   application/zip  for ZIP archive  application/x-tar  for TAR archive  application/octet-stream  for gzipped TAR   200 OK\nContent-Type: application/zip\nTransfer-Encoding: chunked\nContent-Disposition: attachment; filename=\"$ID.zip\"\nX-Response-Time: 13.556ms  The zip file contains a comment with the original URL.  $ unzip -z CXE1c.zip\nArchive:  CXE1c.zip\nCreated by o2r [https://\u2026/api/v1/compendium/CXE1c.zip]",
            "title": "Response"
        },
        {
            "location": "/compendium/download/#error-responses-for-compendium-download",
            "text": "404 Not Found\n\n{\"error\":\"no compendium with this id\"}  400 Bad Request\n\n{\"error\":\"no job found for this compendium, run a job before downloading with image\"}",
            "title": "Error responses for compendium download"
        },
        {
            "location": "/compendium/metadata/",
            "text": "Compendium metadata\n\u00b6\n\n\nBasics\n\u00b6\n\n\nMetadata in a compendium is stored in a directory \n.erc\n. This directory contains the normative metadata documents using a file naming scheme \n<PREFIX>_<MODEL>_<VERSION>.<FORMAT>\n filled via each metadata mapping file found in the broker tool of the o2r metadata tool suite, the default prefix is \nmetadata\n, e.g. \nmetadata_o2r_1.json\n, \nmetadata_zenodo_1.json\n, or \nmetadata_datacite_41.xml\n. The filename of the extracted raw metadata has no versioning and is constantly found as \nmetadata_raw.json\n.\n\n\nA copy of the files in this directory is kept in database for easier access, so every compendium returned by the API can contain different sub-properties in the metadata property.\n\nThis API always returns the database copy of the metadata elements.\n\nYou can download the respective files to access the normative metadata documents.\n\n\nMetadata formats\n\u00b6\n\n\nThe files are available on demand, but metadata variants are created after each metadata update.\n\n\nThe sub-properties of the \nmetadata\n and their content are\n\n\n\n\nraw\n contains raw metadata extracted automatically\n\n\no2r\n holds the \nmain information for display\n and is modelled according the the o2r metadata model. This metadata is reviewed by the user and the basis for translating to other metadata formats and also for \nsearch\n.\n\n\nzenodo\n holds \nZenodo\n metadata for shipments made to Zenodo and is brokered from \no2r\n metadata\n\n\nzenodo_sandbox\n holds \nZenodo\n metadata for shipments made to Zenodo Sandbox, i.e. a clone of \nzenodo\n metadata\n\n\n\n\n\n\nNote\n\n\nThe information in each sub-property are subject to independent workflows and may differ from one another.\nThe term \nbrokering\n is used for translation from one metadata format into another.\n\n\n\n\nMetadata validation\n\u00b6\n\n\nOnly valid metadata can be saved to a compendium.\nThe \no2r\n metadata element is validated against a \nJSON Schema\n using the \nvalidate\n tool of \no2r-meta\n.\nThe schema file is included in the \no2r-meta\n repository: \nhttps://raw.githubusercontent.com/o2r-project/o2r-meta/master/schema/json/o2r-meta-schema.json\n.\n\n\nGet all compendium metadata\n\u00b6\n\n\ncurl https://\u2026/api/v1/$ID\n\n\nGET /api/v1/compendium/:id\n\n\nAbbreviated example response:\n\n\n200 OK\n\n{\n  \"id\":\"12345\",\n  \"metadata\": {\n    \"raw\": {\n      \"title\": \"Programming with Data. Springer, New York, 1998. ISBN 978-0-387-98503-9.\",\n      \"author\": \"John M. Chambers\",\n      \u2026\n    },\n    \"o2r\": {\n      \"title\": \"Programming with Data\",\n      \"creators\": [\n          {\n              \"name\": \"John M. Chambers\"\n          }\n      ],\n      \"publication_date\": 1998,\n      \u2026\n    },\n    \"zenodo\": {\n      \u2026\n    }\n  },\n  \"created\": \u2026,\n  \"files\": \u2026\n}\n\n\n\n\nGet o2r metadata\n\u00b6\n\n\nThe following endpoint allows to access \nonly\n the normative o2r-metadata element:\n\n\ncurl https://\u2026/api/v1/$ID/metadata\n\n\nGET /api/v1/compendium/:id/metadata\n\n\n200 OK\n\n{\n  \"id\":\"compendium_id\",\n  \"metadata\": {\n    \"o2r\": {\n      \u2026\n    }\n  }\n}\n\n\n\n\nURL parameters\n\u00b6\n\n\n\n\n:id\n - compendium id\n\n\n\n\nSpatial metadata\n\u00b6\n\n\nFor discovery purposes, the metadata includes extracted \nGeoJSON\n bounding boxes based on data files in a workspace.\n\n\nCurrently supported spatial data sources:\n\n\n\n\nshapefiles\n\n\n\n\nThe following structure is made available per file:\n\n\n    \"spatial\": {\n        \"files\": [\n            {\n                \"geojson\": {\n                    \"bbox\": [\n                        -2.362060546875,\n                        52.0862573323384,\n                        -1.285400390625,\n                        52.649729197309426\n                    ],\n                    \"geometry\": {\n                        \"coordinates\": [\n                            [\n                                [\n                                    -2.362060546875,\n                                    52.0862573323384\n                                ],\n                                [\n                                    -1.285400390625,\n                                    52.649729197309426\n                                ]\n                            ]\n                        ],\n                        \"type\": \"Polygon\"\n                    },\n                    \"type\": \"Feature\"\n                },\n                \"source_file\": \"path/to/file1.geojson\"\n            },\n            {\n                \"geojson\": {\n                    \"bbox\": [\n                        7.595369517803192,\n                        51.96245837645124,\n                        7.62162297964096,\n                        51.96966694957956\n                    ],\n                    \"geometry\": {\n                        \"coordinates\": [\n                            [\n                                [\n                                    7.595369517803192,\n                                    51.96245837645124\n                                ],\n                                [\n                                    7.62162297964096,\n                                    51.96966694957956\n                                ]\n                            ]\n                        ],\n                        \"type\": \"Polygon\"\n                    },\n                    \"type\": \"Feature\"\n                },\n                \"source_file\": \"path/to/file2.shp\"\n            }\n        ],\n        \"union\": {\n            \"geojson\": {\n                \"bbox\": [\n                    -2.362060546875,\n                    51.96245837645124,\n                    7.62162297964096,\n                    51.96245837645124\n                ],\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            -2.362060546875,\n                            51.96245837645124\n                        ],\n                        [\n                            7.62162297964096,\n                            51.96245837645124\n                        ],\n                        [\n                            7.62162297964096,\n                            52.649729197309426\n                        ],\n                        [\n                            -2.362060546875,\n                            52.649729197309426\n                        ]\n                    ],\n                    \"type\": \"Polygon\"\n                },\n                \"type\": \"Feature\"\n            }\n        }\n    }\n\n\n\n\nThe \nspatial\n key has a \nunion\n bounding box, that wraps all extracted bounding boxes.\n\n\nUpdate metadata\n\u00b6\n\n\nThe following endpoint can be used to update the \no2r\n metadata elements.\nAll other metadata sub-properties are only updated by the service itself, i.e. brokered metadata.\nAfter creation the metadata is persisted to both files and database, so updating the metadata via this endpoint allows to trigger a brokering process and to retrieve different metadata formats either via this metadata API or via downloading the respective file using the \ndownload endpoint\n.\n\n\n\n\nMetadata update rights\n\n\nOnly authors of a compendium or users with the required \nuser level\n can update a compendium's metadata.\n\n\n\n\nMetadata update request\n\u00b6\n\n\ncurl -H 'Content-Type: application/json' \\\n  -X PUT \\\n  --cookie \"connect.sid=<code string here>\" \\\n  -d '{ \"o2r\": { \"title\": \"Blue Book\" } }' \\\n  /api/v1/compendium/:id/metadata\n\n\n\n\nThe request \noverwrites\n the existing metadata properties, so the \nfull\n o2r metadata must be put with a JSON object called \no2r\n at the root, even if only specific fields are changed.\n\n\n\n\nNote\n\n\nThis endpoint allows only to update the \nmetadata.o2r\n elements. All other properties of \n\n\n\n\nURL parameters\n\u00b6\n\n\n\n\n:id\n - compendium id\n\n\n\n\nMetadata update response\n\u00b6\n\n\nThe response contains an excerpt of a compendium with only the o2r metadata property.\n\n\n200 OK\n\n{\n  \"id\":\"compendium_id\",\n  \"metadata\": {\n    \"o2r\": {\n      \"title\": \"Blue Book\"\n    }\n  }\n}\n\n\n\n\nMetadata update error responses\n\u00b6\n\n\n401 Unauthorized\n\n{\"error\":\"not authorized\"}\n\n\n\n\n400 Incomplete metadata (description property missing)\n\n{\n    \"error\":\"Error updating metadata file, see log for details\",\n    \"log\": \"[o2rmeta] 20180302.085940 received arguments: {'debug': True, 'tool': 'validate', 'schema': 'schema/json/o2r-meta-schema.json', 'candidate': '/tmp/o2r/compendium/1cAIr/data/.erc/metadata_o2r_1.json'}\n    [o2rmeta] 20180302.085940 launching validator\n    [o2rmeta] 20180302.085940 checking metadata_o2r_1.json against o2r-meta-schema.json\n    [o2rmeta] 20180302.085940 !invalid: None is not of type 'string'\n\n    Failed validating 'type' in schema['properties']['description']:\n        {'type': 'string'}\n\n        On instance['description']:\n            None\"\n}\n\n\n\n\n400 Bad Request\n\n\"SyntaxError [...]\"\n\n\n\n\n422 Unprocessable Entity\n\n{\"error\":\"JSON with root element 'o2r' required\"}\n\n\n\n\nOther metadata properties\n\u00b6\n\n\nBesides the \nmetadata\n element, a compendium persists some additional properties to reduce computation on the server, and to allows client applications to improve the user experience.\n\n\n\n\nbag\n - a boolean showing if the uploaded artefact was detected as a BagIt bag (detection file: \nbagit.txt\n)\n\n\ncompendium\n - a boolean showing if the uploaded artefact was detected as a compendium (detection file: \nerc.yml\n)\n\n\n\n\nExample:\n\n\n(Properties \nmetadata\n and \nfiles\n not shown for brevity.)\n\n\n{\n\n    \"id\": \"U9IZ7\",\n    \"metadata\": {},\n    \"created\": \"2017-01-01T00:00:42.000Z\",\n    \"user\": \"0000-0002-1825-0097\",\n    \"bag\": false,\n    \"compendium\": false,\n    \"files\": {}\n}",
            "title": "Metadata"
        },
        {
            "location": "/compendium/metadata/#compendium-metadata",
            "text": "",
            "title": "Compendium metadata"
        },
        {
            "location": "/compendium/metadata/#basics",
            "text": "Metadata in a compendium is stored in a directory  .erc . This directory contains the normative metadata documents using a file naming scheme  <PREFIX>_<MODEL>_<VERSION>.<FORMAT>  filled via each metadata mapping file found in the broker tool of the o2r metadata tool suite, the default prefix is  metadata , e.g.  metadata_o2r_1.json ,  metadata_zenodo_1.json , or  metadata_datacite_41.xml . The filename of the extracted raw metadata has no versioning and is constantly found as  metadata_raw.json .  A copy of the files in this directory is kept in database for easier access, so every compendium returned by the API can contain different sub-properties in the metadata property. This API always returns the database copy of the metadata elements. \nYou can download the respective files to access the normative metadata documents.",
            "title": "Basics"
        },
        {
            "location": "/compendium/metadata/#metadata-formats",
            "text": "The files are available on demand, but metadata variants are created after each metadata update.  The sub-properties of the  metadata  and their content are   raw  contains raw metadata extracted automatically  o2r  holds the  main information for display  and is modelled according the the o2r metadata model. This metadata is reviewed by the user and the basis for translating to other metadata formats and also for  search .  zenodo  holds  Zenodo  metadata for shipments made to Zenodo and is brokered from  o2r  metadata  zenodo_sandbox  holds  Zenodo  metadata for shipments made to Zenodo Sandbox, i.e. a clone of  zenodo  metadata    Note  The information in each sub-property are subject to independent workflows and may differ from one another.\nThe term  brokering  is used for translation from one metadata format into another.",
            "title": "Metadata formats"
        },
        {
            "location": "/compendium/metadata/#metadata-validation",
            "text": "Only valid metadata can be saved to a compendium.\nThe  o2r  metadata element is validated against a  JSON Schema  using the  validate  tool of  o2r-meta .\nThe schema file is included in the  o2r-meta  repository:  https://raw.githubusercontent.com/o2r-project/o2r-meta/master/schema/json/o2r-meta-schema.json .",
            "title": "Metadata validation"
        },
        {
            "location": "/compendium/metadata/#get-all-compendium-metadata",
            "text": "curl https://\u2026/api/v1/$ID  GET /api/v1/compendium/:id  Abbreviated example response:  200 OK\n\n{\n  \"id\":\"12345\",\n  \"metadata\": {\n    \"raw\": {\n      \"title\": \"Programming with Data. Springer, New York, 1998. ISBN 978-0-387-98503-9.\",\n      \"author\": \"John M. Chambers\",\n      \u2026\n    },\n    \"o2r\": {\n      \"title\": \"Programming with Data\",\n      \"creators\": [\n          {\n              \"name\": \"John M. Chambers\"\n          }\n      ],\n      \"publication_date\": 1998,\n      \u2026\n    },\n    \"zenodo\": {\n      \u2026\n    }\n  },\n  \"created\": \u2026,\n  \"files\": \u2026\n}",
            "title": "Get all compendium metadata"
        },
        {
            "location": "/compendium/metadata/#get-o2r-metadata",
            "text": "The following endpoint allows to access  only  the normative o2r-metadata element:  curl https://\u2026/api/v1/$ID/metadata  GET /api/v1/compendium/:id/metadata  200 OK\n\n{\n  \"id\":\"compendium_id\",\n  \"metadata\": {\n    \"o2r\": {\n      \u2026\n    }\n  }\n}",
            "title": "Get o2r metadata"
        },
        {
            "location": "/compendium/metadata/#url-parameters",
            "text": ":id  - compendium id",
            "title": "URL parameters"
        },
        {
            "location": "/compendium/metadata/#spatial-metadata",
            "text": "For discovery purposes, the metadata includes extracted  GeoJSON  bounding boxes based on data files in a workspace.  Currently supported spatial data sources:   shapefiles   The following structure is made available per file:      \"spatial\": {\n        \"files\": [\n            {\n                \"geojson\": {\n                    \"bbox\": [\n                        -2.362060546875,\n                        52.0862573323384,\n                        -1.285400390625,\n                        52.649729197309426\n                    ],\n                    \"geometry\": {\n                        \"coordinates\": [\n                            [\n                                [\n                                    -2.362060546875,\n                                    52.0862573323384\n                                ],\n                                [\n                                    -1.285400390625,\n                                    52.649729197309426\n                                ]\n                            ]\n                        ],\n                        \"type\": \"Polygon\"\n                    },\n                    \"type\": \"Feature\"\n                },\n                \"source_file\": \"path/to/file1.geojson\"\n            },\n            {\n                \"geojson\": {\n                    \"bbox\": [\n                        7.595369517803192,\n                        51.96245837645124,\n                        7.62162297964096,\n                        51.96966694957956\n                    ],\n                    \"geometry\": {\n                        \"coordinates\": [\n                            [\n                                [\n                                    7.595369517803192,\n                                    51.96245837645124\n                                ],\n                                [\n                                    7.62162297964096,\n                                    51.96966694957956\n                                ]\n                            ]\n                        ],\n                        \"type\": \"Polygon\"\n                    },\n                    \"type\": \"Feature\"\n                },\n                \"source_file\": \"path/to/file2.shp\"\n            }\n        ],\n        \"union\": {\n            \"geojson\": {\n                \"bbox\": [\n                    -2.362060546875,\n                    51.96245837645124,\n                    7.62162297964096,\n                    51.96245837645124\n                ],\n                \"geometry\": {\n                    \"coordinates\": [\n                        [\n                            -2.362060546875,\n                            51.96245837645124\n                        ],\n                        [\n                            7.62162297964096,\n                            51.96245837645124\n                        ],\n                        [\n                            7.62162297964096,\n                            52.649729197309426\n                        ],\n                        [\n                            -2.362060546875,\n                            52.649729197309426\n                        ]\n                    ],\n                    \"type\": \"Polygon\"\n                },\n                \"type\": \"Feature\"\n            }\n        }\n    }  The  spatial  key has a  union  bounding box, that wraps all extracted bounding boxes.",
            "title": "Spatial metadata"
        },
        {
            "location": "/compendium/metadata/#update-metadata",
            "text": "The following endpoint can be used to update the  o2r  metadata elements.\nAll other metadata sub-properties are only updated by the service itself, i.e. brokered metadata.\nAfter creation the metadata is persisted to both files and database, so updating the metadata via this endpoint allows to trigger a brokering process and to retrieve different metadata formats either via this metadata API or via downloading the respective file using the  download endpoint .   Metadata update rights  Only authors of a compendium or users with the required  user level  can update a compendium's metadata.",
            "title": "Update metadata"
        },
        {
            "location": "/compendium/metadata/#metadata-update-request",
            "text": "curl -H 'Content-Type: application/json' \\\n  -X PUT \\\n  --cookie \"connect.sid=<code string here>\" \\\n  -d '{ \"o2r\": { \"title\": \"Blue Book\" } }' \\\n  /api/v1/compendium/:id/metadata  The request  overwrites  the existing metadata properties, so the  full  o2r metadata must be put with a JSON object called  o2r  at the root, even if only specific fields are changed.   Note  This endpoint allows only to update the  metadata.o2r  elements. All other properties of",
            "title": "Metadata update request"
        },
        {
            "location": "/compendium/metadata/#url-parameters_1",
            "text": ":id  - compendium id",
            "title": "URL parameters"
        },
        {
            "location": "/compendium/metadata/#metadata-update-response",
            "text": "The response contains an excerpt of a compendium with only the o2r metadata property.  200 OK\n\n{\n  \"id\":\"compendium_id\",\n  \"metadata\": {\n    \"o2r\": {\n      \"title\": \"Blue Book\"\n    }\n  }\n}",
            "title": "Metadata update response"
        },
        {
            "location": "/compendium/metadata/#metadata-update-error-responses",
            "text": "401 Unauthorized\n\n{\"error\":\"not authorized\"}  400 Incomplete metadata (description property missing)\n\n{\n    \"error\":\"Error updating metadata file, see log for details\",\n    \"log\": \"[o2rmeta] 20180302.085940 received arguments: {'debug': True, 'tool': 'validate', 'schema': 'schema/json/o2r-meta-schema.json', 'candidate': '/tmp/o2r/compendium/1cAIr/data/.erc/metadata_o2r_1.json'}\n    [o2rmeta] 20180302.085940 launching validator\n    [o2rmeta] 20180302.085940 checking metadata_o2r_1.json against o2r-meta-schema.json\n    [o2rmeta] 20180302.085940 !invalid: None is not of type 'string'\n\n    Failed validating 'type' in schema['properties']['description']:\n        {'type': 'string'}\n\n        On instance['description']:\n            None\"\n}  400 Bad Request\n\n\"SyntaxError [...]\"  422 Unprocessable Entity\n\n{\"error\":\"JSON with root element 'o2r' required\"}",
            "title": "Metadata update error responses"
        },
        {
            "location": "/compendium/metadata/#other-metadata-properties",
            "text": "Besides the  metadata  element, a compendium persists some additional properties to reduce computation on the server, and to allows client applications to improve the user experience.   bag  - a boolean showing if the uploaded artefact was detected as a BagIt bag (detection file:  bagit.txt )  compendium  - a boolean showing if the uploaded artefact was detected as a compendium (detection file:  erc.yml )   Example:  (Properties  metadata  and  files  not shown for brevity.)  {\n\n    \"id\": \"U9IZ7\",\n    \"metadata\": {},\n    \"created\": \"2017-01-01T00:00:42.000Z\",\n    \"user\": \"0000-0002-1825-0097\",\n    \"bag\": false,\n    \"compendium\": false,\n    \"files\": {}\n}",
            "title": "Other metadata properties"
        },
        {
            "location": "/compendium/files/",
            "text": "Compendium file listing\n\u00b6\n\n\nThe file listing is returned in the single view of a job or compendium. It includes the complete content of the bagtainer in its current state.\nIf a job has been run and the programme outputs new data, this new data is included as well.\n\n\nFile listings are represented as a Object. The file structure for a synthetic job \nnj141\n is as follows.\n\n\nnj141\n\u251c\u2500\u2500 bagit.txt\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 paper.Rmd\n    \u2514\u2500\u2500 Dockerfile\n\n\n\n\nis be represented as\n\n\n{\n  \"path\": \"/api/v1/job/nj141/data\",\n  \"name\": \"nj141\",\n  \"children\": [\n    {\n      \"path\": \"/api/v1/job/nj141/data/bagit.txt\",\n      \"name\": \"bagit.xt\",\n      \"type\": \"text/plain\",\n      \"size\": 55\n    },\n    {\n      \"path\": \"/api/v1/job/nj141/data/data\",\n      \"name\": \"data\",\n      \"children\": [\n        {\n          \"path\": \"/api/v1/job/nj141/data/data/paper.Rmd\",\n          \"name\": \"paper.Rmd\",\n          \"type\": \"text/plain\",\n          \"size\": 346512\n        }\n        {\n          \"path\": \"/api/v1/job/nj141/data/data/Dockerfile\",\n          \"name\": \"Dockerfile\",\n          \"type\": \"text/plain\",\n          \"size\": 1729\n        }\n      ]\n    }\n  ]\n}\n\n\n\n\npath\n property\n\u00b6\n\n\nThe \npath\n property for each file in the listing is a link to the raw file. Additionally the \nGET\n parameter \n?size=\u2026\n can be appended to retrieve previews of the files. In the case of Images (\npng\n, \njpg\n, \ngif\n, \ntiff\n), the value defines the maximum width/height. For text files (\ntxt\n, \ncsv\n, scripts), the value defines the amount of lines returned.\n\n\ntype\n property\n\u00b6\n\n\nThe \ntype\n property is a best guess for the MIME type of the file content. It is a result of the files extension. Look at the list of extension to type mapping below.\n\n\nFile extension to MIME type mappings\n\u00b6\n\n\nThis list contains the custom mapping of file extensions to MIME types used in the server.\n\n\n\n\n\n\n\n\nExtension\n\n\nMIME type\n\n\n\n\n\n\n\n\n\n\n.R\n, \n.r\n\n\nscript/x-R\n\n\n\n\n\n\n\n\nFile inspection: RData\n\u00b6\n\n\n.RData\n files are a \nbinary format\n for usage with R to save any kind of object (data, functions) using an internal serialisation.\nThe format is \nnot suitable\n for archival or data exchange, but might be included in a compendium out of negligence by or convenience for the author.\n\n\nSince the file format is binary and not readable by non-R client applications, the API provides the endpoint \n/api/v1/inspection\n to retrieve a JSON representation of the objects in an RData file.\n\n\nValues of objects are provided as JSON arrays following the specifications by the R package \njsonlite\n.\n\n\nSimple data types\n\u00b6\n\n\nGET /api/v1/inspection/<compendium id>?file=simple.Rdata\n\n\n200 OK\n\n{  \n  \"aChar\":[  \n    \"a\"\n  ],\n  \"aDouble\":[  \n    2.3\n  ],\n  \"anInteger\":[  \n    1\n  ],\n  \"aString\":[  \n    \"The force is great in o2r.\"\n  ]\n}\n\n\n\n\nComplex data types\n\u00b6\n\n\nLists are be nested objects, and vectors are JSON arrays (see \njsonlite\n docs\n for details, defaults are used):\n\n\nGET /api/v1/inspection/<compendium id>?file=complex.Rdata\n\n\n200 OK\n\n{  \n  \"characterVector\":[  \n    \"one\",\n    \"two\",\n    \"3\"\n  ],\n  \"logicalVector\":[  \n    true,\n    true,\n    false\n  ],\n  \"numericVector\":[  \n    1,\n    2,\n    -7,\n    0.8\n  ],\n  \"orderedList\":{  \n    \"name\":[  \n      \"Fred\"\n    ],\n    \"mynumbers\":[  \n      1,\n      2\n    ],\n    \"age\":[  \n      5.3\n    ]\n  }\n}\n\n\n\n\nData frames and matrices are mapped to JSON arrays of complex objects (see \njsonlite\n docs\n for details, defaults are used):\n\n\nGET /api/v1/inspection/<compendium id>?file=matrices.Rdata\n\n\n200 OK\n\n{  \n  \"dataFrame\":[  \n    {  \n      \"ID\":1,\n      \"Passed\":true,\n      \"Colour\":\"red\"\n    },\n    {  \n      \"ID\":2,\n      \"Passed\":true,\n      \"Colour\":\"white\"\n    },\n    {  \n      \"ID\":3,\n      \"Passed\":true,\n      \"Colour\":\"red\"\n    },\n    {  \n      \"ID\":4,\n      \"Passed\":false\n    }\n  ],\n  \"namedMatrix\":[  \n    [  \n      1,\n      26\n    ],\n    [  \n      24,\n      68\n    ]\n  ]\n}\n\n\n\n\nPath parameters\n\u00b6\n\n\n\n\ncompendium_id\n \nmandatory\n - the compendium identifier to inspect the file from\n\n\n\n\nQuery parameters\n\u00b6\n\n\n\n\nfile\n \nmandatory\n - the name of the file to inspect, or a relative path to a file within the compendium\n\n\nobjects\n \noptional\n - the name of objects in the file\n\n\n\n\nIf selected objects are not loadable from the file, an \nerrors\n property in the response is given for each problematic object:\n\n\nGET /api/v1/inspection/<compendium id>?file=simple.RData&objects=bar,anInteger,foo\n\n\n200 OK\n\n{  \n  \"anInteger\":[  \n    1\n  ],\n  \"errors\":[  \n    \"Error: Object 'bar' does not exist in the file simple.RData\",\n    \"Error: Object 'foo' does not exist in the file simple.RData\"\n  ]\n}\n\n\n\n\nErrors\n\u00b6\n\n\n400 Bad Request\n\n{\"error\": \"Query parameter 'file' missing\"}\n\n\n\n\n400 Bad Request\n\n{\"error\": \"file 'not_available.Rdata' does not exist in compendium kOSMO\"}\n\n\n\n\n400 Bad Request\n\n{\"error\": \"compendium '12345' does not exist\"}\n\n\n\n\n500 Internal Server Error\n\n{\"error\": \"Error loading objects\"}",
            "title": "Files in a compendium"
        },
        {
            "location": "/compendium/files/#compendium-file-listing",
            "text": "The file listing is returned in the single view of a job or compendium. It includes the complete content of the bagtainer in its current state.\nIf a job has been run and the programme outputs new data, this new data is included as well.  File listings are represented as a Object. The file structure for a synthetic job  nj141  is as follows.  nj141\n\u251c\u2500\u2500 bagit.txt\n\u2514\u2500\u2500 data\n    \u251c\u2500\u2500 paper.Rmd\n    \u2514\u2500\u2500 Dockerfile  is be represented as  {\n  \"path\": \"/api/v1/job/nj141/data\",\n  \"name\": \"nj141\",\n  \"children\": [\n    {\n      \"path\": \"/api/v1/job/nj141/data/bagit.txt\",\n      \"name\": \"bagit.xt\",\n      \"type\": \"text/plain\",\n      \"size\": 55\n    },\n    {\n      \"path\": \"/api/v1/job/nj141/data/data\",\n      \"name\": \"data\",\n      \"children\": [\n        {\n          \"path\": \"/api/v1/job/nj141/data/data/paper.Rmd\",\n          \"name\": \"paper.Rmd\",\n          \"type\": \"text/plain\",\n          \"size\": 346512\n        }\n        {\n          \"path\": \"/api/v1/job/nj141/data/data/Dockerfile\",\n          \"name\": \"Dockerfile\",\n          \"type\": \"text/plain\",\n          \"size\": 1729\n        }\n      ]\n    }\n  ]\n}",
            "title": "Compendium file listing"
        },
        {
            "location": "/compendium/files/#path-property",
            "text": "The  path  property for each file in the listing is a link to the raw file. Additionally the  GET  parameter  ?size=\u2026  can be appended to retrieve previews of the files. In the case of Images ( png ,  jpg ,  gif ,  tiff ), the value defines the maximum width/height. For text files ( txt ,  csv , scripts), the value defines the amount of lines returned.",
            "title": "path property"
        },
        {
            "location": "/compendium/files/#type-property",
            "text": "The  type  property is a best guess for the MIME type of the file content. It is a result of the files extension. Look at the list of extension to type mapping below.",
            "title": "type property"
        },
        {
            "location": "/compendium/files/#file-extension-to-mime-type-mappings",
            "text": "This list contains the custom mapping of file extensions to MIME types used in the server.     Extension  MIME type      .R ,  .r  script/x-R",
            "title": "File extension to MIME type mappings"
        },
        {
            "location": "/compendium/files/#file-inspection-rdata",
            "text": ".RData  files are a  binary format  for usage with R to save any kind of object (data, functions) using an internal serialisation.\nThe format is  not suitable  for archival or data exchange, but might be included in a compendium out of negligence by or convenience for the author.  Since the file format is binary and not readable by non-R client applications, the API provides the endpoint  /api/v1/inspection  to retrieve a JSON representation of the objects in an RData file.  Values of objects are provided as JSON arrays following the specifications by the R package  jsonlite .",
            "title": "File inspection: RData"
        },
        {
            "location": "/compendium/files/#simple-data-types",
            "text": "GET /api/v1/inspection/<compendium id>?file=simple.Rdata  200 OK\n\n{  \n  \"aChar\":[  \n    \"a\"\n  ],\n  \"aDouble\":[  \n    2.3\n  ],\n  \"anInteger\":[  \n    1\n  ],\n  \"aString\":[  \n    \"The force is great in o2r.\"\n  ]\n}",
            "title": "Simple data types"
        },
        {
            "location": "/compendium/files/#complex-data-types",
            "text": "Lists are be nested objects, and vectors are JSON arrays (see  jsonlite  docs  for details, defaults are used):  GET /api/v1/inspection/<compendium id>?file=complex.Rdata  200 OK\n\n{  \n  \"characterVector\":[  \n    \"one\",\n    \"two\",\n    \"3\"\n  ],\n  \"logicalVector\":[  \n    true,\n    true,\n    false\n  ],\n  \"numericVector\":[  \n    1,\n    2,\n    -7,\n    0.8\n  ],\n  \"orderedList\":{  \n    \"name\":[  \n      \"Fred\"\n    ],\n    \"mynumbers\":[  \n      1,\n      2\n    ],\n    \"age\":[  \n      5.3\n    ]\n  }\n}  Data frames and matrices are mapped to JSON arrays of complex objects (see  jsonlite  docs  for details, defaults are used):  GET /api/v1/inspection/<compendium id>?file=matrices.Rdata  200 OK\n\n{  \n  \"dataFrame\":[  \n    {  \n      \"ID\":1,\n      \"Passed\":true,\n      \"Colour\":\"red\"\n    },\n    {  \n      \"ID\":2,\n      \"Passed\":true,\n      \"Colour\":\"white\"\n    },\n    {  \n      \"ID\":3,\n      \"Passed\":true,\n      \"Colour\":\"red\"\n    },\n    {  \n      \"ID\":4,\n      \"Passed\":false\n    }\n  ],\n  \"namedMatrix\":[  \n    [  \n      1,\n      26\n    ],\n    [  \n      24,\n      68\n    ]\n  ]\n}",
            "title": "Complex data types"
        },
        {
            "location": "/compendium/files/#path-parameters",
            "text": "compendium_id   mandatory  - the compendium identifier to inspect the file from",
            "title": "Path parameters"
        },
        {
            "location": "/compendium/files/#query-parameters",
            "text": "file   mandatory  - the name of the file to inspect, or a relative path to a file within the compendium  objects   optional  - the name of objects in the file   If selected objects are not loadable from the file, an  errors  property in the response is given for each problematic object:  GET /api/v1/inspection/<compendium id>?file=simple.RData&objects=bar,anInteger,foo  200 OK\n\n{  \n  \"anInteger\":[  \n    1\n  ],\n  \"errors\":[  \n    \"Error: Object 'bar' does not exist in the file simple.RData\",\n    \"Error: Object 'foo' does not exist in the file simple.RData\"\n  ]\n}",
            "title": "Query parameters"
        },
        {
            "location": "/compendium/files/#errors",
            "text": "400 Bad Request\n\n{\"error\": \"Query parameter 'file' missing\"}  400 Bad Request\n\n{\"error\": \"file 'not_available.Rdata' does not exist in compendium kOSMO\"}  400 Bad Request\n\n{\"error\": \"compendium '12345' does not exist\"}  500 Internal Server Error\n\n{\"error\": \"Error loading objects\"}",
            "title": "Errors"
        },
        {
            "location": "/compendium/substitute/",
            "text": "Substitution\n\u00b6\n\n\nSubstitution is the combination of an base compendium, \"base\" for short, and an overlay compendium, or \"overlay\".\nA user can choose files from the overlay to replace files of the base, or upload new files.\nAdditionally the user can choose, if the metadata of the base ERC will be adopted for substitution (\nkeepBase\n) or there will be a new extraction of the metadata for the substituted ERC.\nThis new extraction is divided into two choices.\nThe user can let the new extracted metadata be merged into the existing metadata of the base ERC (\nextractAndMerge\n - \nnot implemented\n) or just save the extracted metadata (\nextract\n - \nnot implemented\n).\n\n\nCreate substitution\n\u00b6\n\n\nCreate substitution\n produces a new compendium with its own files in the storage and metadata in the database.\nA substitution can be created with an HTTP \nPOST\n request using \nmultipart/form-data\n and content-type \nJSON\n.\nRequired content of the request are the identifiers of the base and overlay compendia and at least one pair of \nsubstitution files\n, consisting of a base file and an overlay file.\n\n\n\n\nNote\n\n\nA substitution process removes potentially existing packaging information, i.e. if the base compendium was a BagIt bag, the substitution will only contain the payload directory contents (\n/data\n directory).\n\n\nThe overlay file is stripped of all paths and is copied directly into the substitution's root directory.\n\n\n\n\nRequest\n\u00b6\n\n\nPOST /api/v1/substitution\n\n\nRequest body for a new substitution:\n\n\n{\n  \"base\": \"G92NL\",\n  \"overlay\": \"9fCTR\",\n  \"substitutionFiles\": [\n    {\n      \"base\": \"climate-timeseries.csv\",\n      \"overlay\": \"mytimeseries_data.csv\"\n    }\n  ],\n  \"metadataHandling\": \"keepBase\"\n}\n\n\n\n\nRequest body properties\n\u00b6\n\n\n\n\nbase\n - id of the base compendium\n\n\noverlay\n - id of the overlay compendium\n\n\nsubstitutionFiles\n - array of file substitutions specified by \nbase\n and \noverlay\n\n\nbase\n - name of the file from the base compendium\n\n\noverlay\n - name of the overlay compendium that is exchanged for the original file\n\n\nmetadataHandling\n - property to specify, if the metadata of the base ERC will be adopted (\nkeepBase\n = \nkeep metadata\n of base ERC) or there will be a new extraction of metadata, that will be merged into the metadata of the base ERC (\nextractAndMerge\n = \nextract and merge metadata\n for new ERC) or that will not be merged (\nextract\n = \nextract metadata\n of new ERC)\n\n\n\n\n\n\nRequired user level\n\n\nThe user creating a new substitution must have the required \nuser level\n.\n\n\n\n\nResponse\n\u00b6\n\n\n201 CREATED\n\n{\n  \"id\": \"oMMFn\"\n}\n\n\n\n\nError responses\n\u00b6\n\n\n401 Unauthorized\n\n{\"error\":\"not authenticated\"}\n\n\n\n\n401 Unauthorized\n\n{\"error\":\"not allowed\"}\n\n\n\n\n404 Not Found\n\n{\"error\":\"base compendium not found\"}\n\n\n\n\n404 Not Found\n\n{\"error\":\"overlay compendium not found\"}\n\n\n\n\nView substituted Compendium\n\u00b6\n\n\nRequest\n\u00b6\n\n\ncurl https://.../api/v1/compendium/$ID\n\n\nGET /api/v1/compendium/:id\n\n\nThis request is handled as regular GET request of a compendium (see \nView single compendium\n).\n\n\nResponse\n\u00b6\n\n\nA substituted compendium is be saved as a usual compendium, but with additional metadata specifying this as a substituted compendium and giving information about the substitution.\n\n\nExample 01 - in case there are no conflicts between filenames of any base file and overlay file :\n\n\n200 OK\n\n{\n  \"id\": \"oMMFn\",\n  ...\n  \"metadata\": {\n      ...\n      \"substitution\": {\n          \"base\": \"G92NL\",\n          \"overlay\": \"9fCTR\",\n          \"substitutionFiles\": [\n            {\n              \"base\": \"climate-timeseries.csv\",\n              \"overlay\": \"mytimeseries_data.csv\",\n              \"filename\": \"climate-timeseries.csv\"\n            }\n          ],\n          \"metadataHandling\": \"keepBase\"\n      },\n      ...\n  },\n  \"substituted\": true,\n  ...\n}\n\n\n\n\nExample 02 - in case the overlay file has the same filename as one of the existing base files and is in a sub-directory in the overlay compendium:\n\n\n200 OK\n\n{\n  \"id\": \"oMMFn\",\n  ...\n  \"metadata\": {\n      ...\n      \"substitution\": {\n          \"base\": \"G92NL\",\n          \"overlay\": \"9fCTR\",\n          \"substitutionFiles\": [\n            {\n              \"base\": \"climate-timeseries.csv\",\n              \"overlay\": \"dataFiles/input.csv\",\n              \"filename\": \"overlay_input.csv\"\n            }\n          ],\n          \"metadataHandling\": \"keepBase\"\n      },\n      ...\n  },\n  \"substituted\": true,\n  ...\n}\n\n\n\n\nResponse additional metadata\n\u00b6\n\n\n\n\nmetadata.substitution\n - object, specifying information about the substitution\n\n\nbase\n - id of the base compendium\n\n\noverlay\n - id of the overlay compendium\n\n\nsubstitutionFiles\n - array of file substitutions specified by \nbase\n and \noverlay\n\n\nbase\n - name of the file from the base ERC\n\n\noverlay\n - name of the file from the overlay ERC\n\n\nfilename\n - as seen in the examples above, \nfilename\n will be created.\nIf there is a conflict with any basefilename and an overlayfilename, the overlayfilename will get an additional \"\noverlay_\n\" prepended (see Example 02). \n(optional add)\n\n\n\n\n\n\nmetadataHandling\n - property to specify, if the metadata of the base ERC will be adopted (\nkeepBase\n = \nkeep metadata\n of base ERC) or there will be a new extraction of metadata, that will be merged into the metadata of the base ERC (\nextractAndMerge\n = \nextract and merge metadata\n for new ERC) or that will not be merged (\nextract\n = \nextract metadata\n of new ERC)\n\n\n\n\n\n\nsubstituted\n - will be set \ntrue\n\n\n\n\nList substituted Compendia\n\u00b6\n\n\nRequest\n\u00b6\n\n\ncurl https://.../api/v1/substitution\n\n\nGET /api/v1/substitution\n\n\nResponse\n\u00b6\n\n\nThe result is a list of compendia ids which were created by a substitution process.\n\n\n200 OK\n\n{\n  \"results\": [\n    \"oMMFn\",\n    \"asdi5\",\n    \"nb2sg\",\n    \u2026\n  ]\n}\n\n\n\n\nIf there are no substitutions yet, the returned list is empty.\n\n\n200 OK\n{\n  \"results\": [ ]\n}\n\n\n\n\nFilter results with following parameters:\n\u00b6\n\n\ncurl https://.../api/v1/substitution?base=$BASE_ID&overlay=$OVERLAY_ID\n\n\nGET /api/v1/substitution?base=base_id&overlay=overlay_id\n\n\n\n\nFilter by \nbase\n:\n\n\n\n\ncurl https://.../api/v1/substitution?base=jfL3w\n\n\nGET /api/v1/substitution?base=jfL3w\n\n\nResult is a list of substituted compendia based on the given base compendium:\n\n\n200 OK\n\n{\n  \"results\": [\n    \"wGmFn\",\n    \u2026\n  ]\n}\n\n\n\n\n\n\nFilter by \noverlay\n:\n\n\n\n\ncurl https://.../api/v1/substitution?overlay=as4Kj\n\n\nGET /api/v1/substitution?overlay=as4Kj\n\n\nResult is a list of substituted compendia based on the given overlay compendium:\n\n\n200 OK\n\n{\n  \"results\": [\n    \"9pQ34\",\n    \"1Tnd3\",\n    \u2026\n  ]\n}\n\n\n\n\n\n\nFilter by \nbase\n and \noverlay\n:\n\n\n\n\ncurl https://.../api/v1/substitution?base=lO3Td&overlay=as4Kj\n\n\nGET /api/v1/substitution?base=lO3Td&overlay=as4Kj\n\n\nResult is a list of substituted compendia based on the given base \nand\n overlay compendium:\n\n\n200 OK\n\n{\n  \"results\": [\n    \"9pQ34\",\n    \u2026\n  ]\n}\n\n\n\n\nError responses\n\u00b6\n\n\n401 Unauthorized\n\n{\"error\":\"not authenticated\"}\n\n\n\n\n401 Unauthorized\n\n{\"error\":\"not allowed\"}\n\n\n\n\n404 Not Found\n\n{\"error\":\"base compendium not found\"}\n\n\n\n\n404 Not Found\n\n{\"error\":\"overlay compendium not found\"}\n\n\n\n\n400 Not Found\n\n{\"error\":\"base file is undefined\"}\n\n\n\n\n400 Not Found\n\n{\"error\":\"overlay file is undefined\"}\n\n\n\n\nURL parameters for substituted compendium lists\n\u00b6\n\n\n\n\n:base\n - id of the base compendium that the results should be related to\n\n\n:overlay\n - id of the overlay compendium that the results should be related to",
            "title": "Substitution"
        },
        {
            "location": "/compendium/substitute/#substitution",
            "text": "Substitution is the combination of an base compendium, \"base\" for short, and an overlay compendium, or \"overlay\".\nA user can choose files from the overlay to replace files of the base, or upload new files.\nAdditionally the user can choose, if the metadata of the base ERC will be adopted for substitution ( keepBase ) or there will be a new extraction of the metadata for the substituted ERC.\nThis new extraction is divided into two choices.\nThe user can let the new extracted metadata be merged into the existing metadata of the base ERC ( extractAndMerge  -  not implemented ) or just save the extracted metadata ( extract  -  not implemented ).",
            "title": "Substitution"
        },
        {
            "location": "/compendium/substitute/#create-substitution",
            "text": "Create substitution  produces a new compendium with its own files in the storage and metadata in the database.\nA substitution can be created with an HTTP  POST  request using  multipart/form-data  and content-type  JSON .\nRequired content of the request are the identifiers of the base and overlay compendia and at least one pair of  substitution files , consisting of a base file and an overlay file.   Note  A substitution process removes potentially existing packaging information, i.e. if the base compendium was a BagIt bag, the substitution will only contain the payload directory contents ( /data  directory).  The overlay file is stripped of all paths and is copied directly into the substitution's root directory.",
            "title": "Create substitution"
        },
        {
            "location": "/compendium/substitute/#request",
            "text": "POST /api/v1/substitution  Request body for a new substitution:  {\n  \"base\": \"G92NL\",\n  \"overlay\": \"9fCTR\",\n  \"substitutionFiles\": [\n    {\n      \"base\": \"climate-timeseries.csv\",\n      \"overlay\": \"mytimeseries_data.csv\"\n    }\n  ],\n  \"metadataHandling\": \"keepBase\"\n}",
            "title": "Request"
        },
        {
            "location": "/compendium/substitute/#request-body-properties",
            "text": "base  - id of the base compendium  overlay  - id of the overlay compendium  substitutionFiles  - array of file substitutions specified by  base  and  overlay  base  - name of the file from the base compendium  overlay  - name of the overlay compendium that is exchanged for the original file  metadataHandling  - property to specify, if the metadata of the base ERC will be adopted ( keepBase  =  keep metadata  of base ERC) or there will be a new extraction of metadata, that will be merged into the metadata of the base ERC ( extractAndMerge  =  extract and merge metadata  for new ERC) or that will not be merged ( extract  =  extract metadata  of new ERC)    Required user level  The user creating a new substitution must have the required  user level .",
            "title": "Request body properties"
        },
        {
            "location": "/compendium/substitute/#response",
            "text": "201 CREATED\n\n{\n  \"id\": \"oMMFn\"\n}",
            "title": "Response"
        },
        {
            "location": "/compendium/substitute/#error-responses",
            "text": "401 Unauthorized\n\n{\"error\":\"not authenticated\"}  401 Unauthorized\n\n{\"error\":\"not allowed\"}  404 Not Found\n\n{\"error\":\"base compendium not found\"}  404 Not Found\n\n{\"error\":\"overlay compendium not found\"}",
            "title": "Error responses"
        },
        {
            "location": "/compendium/substitute/#view-substituted-compendium",
            "text": "",
            "title": "View substituted Compendium"
        },
        {
            "location": "/compendium/substitute/#request_1",
            "text": "curl https://.../api/v1/compendium/$ID  GET /api/v1/compendium/:id  This request is handled as regular GET request of a compendium (see  View single compendium ).",
            "title": "Request"
        },
        {
            "location": "/compendium/substitute/#response_1",
            "text": "A substituted compendium is be saved as a usual compendium, but with additional metadata specifying this as a substituted compendium and giving information about the substitution.  Example 01 - in case there are no conflicts between filenames of any base file and overlay file :  200 OK\n\n{\n  \"id\": \"oMMFn\",\n  ...\n  \"metadata\": {\n      ...\n      \"substitution\": {\n          \"base\": \"G92NL\",\n          \"overlay\": \"9fCTR\",\n          \"substitutionFiles\": [\n            {\n              \"base\": \"climate-timeseries.csv\",\n              \"overlay\": \"mytimeseries_data.csv\",\n              \"filename\": \"climate-timeseries.csv\"\n            }\n          ],\n          \"metadataHandling\": \"keepBase\"\n      },\n      ...\n  },\n  \"substituted\": true,\n  ...\n}  Example 02 - in case the overlay file has the same filename as one of the existing base files and is in a sub-directory in the overlay compendium:  200 OK\n\n{\n  \"id\": \"oMMFn\",\n  ...\n  \"metadata\": {\n      ...\n      \"substitution\": {\n          \"base\": \"G92NL\",\n          \"overlay\": \"9fCTR\",\n          \"substitutionFiles\": [\n            {\n              \"base\": \"climate-timeseries.csv\",\n              \"overlay\": \"dataFiles/input.csv\",\n              \"filename\": \"overlay_input.csv\"\n            }\n          ],\n          \"metadataHandling\": \"keepBase\"\n      },\n      ...\n  },\n  \"substituted\": true,\n  ...\n}",
            "title": "Response"
        },
        {
            "location": "/compendium/substitute/#response-additional-metadata",
            "text": "metadata.substitution  - object, specifying information about the substitution  base  - id of the base compendium  overlay  - id of the overlay compendium  substitutionFiles  - array of file substitutions specified by  base  and  overlay  base  - name of the file from the base ERC  overlay  - name of the file from the overlay ERC  filename  - as seen in the examples above,  filename  will be created.\nIf there is a conflict with any basefilename and an overlayfilename, the overlayfilename will get an additional \" overlay_ \" prepended (see Example 02).  (optional add)    metadataHandling  - property to specify, if the metadata of the base ERC will be adopted ( keepBase  =  keep metadata  of base ERC) or there will be a new extraction of metadata, that will be merged into the metadata of the base ERC ( extractAndMerge  =  extract and merge metadata  for new ERC) or that will not be merged ( extract  =  extract metadata  of new ERC)    substituted  - will be set  true",
            "title": "Response additional metadata"
        },
        {
            "location": "/compendium/substitute/#list-substituted-compendia",
            "text": "",
            "title": "List substituted Compendia"
        },
        {
            "location": "/compendium/substitute/#request_2",
            "text": "curl https://.../api/v1/substitution  GET /api/v1/substitution",
            "title": "Request"
        },
        {
            "location": "/compendium/substitute/#response_2",
            "text": "The result is a list of compendia ids which were created by a substitution process.  200 OK\n\n{\n  \"results\": [\n    \"oMMFn\",\n    \"asdi5\",\n    \"nb2sg\",\n    \u2026\n  ]\n}  If there are no substitutions yet, the returned list is empty.  200 OK\n{\n  \"results\": [ ]\n}",
            "title": "Response"
        },
        {
            "location": "/compendium/substitute/#filter-results-with-following-parameters",
            "text": "curl https://.../api/v1/substitution?base=$BASE_ID&overlay=$OVERLAY_ID  GET /api/v1/substitution?base=base_id&overlay=overlay_id   Filter by  base :   curl https://.../api/v1/substitution?base=jfL3w  GET /api/v1/substitution?base=jfL3w  Result is a list of substituted compendia based on the given base compendium:  200 OK\n\n{\n  \"results\": [\n    \"wGmFn\",\n    \u2026\n  ]\n}   Filter by  overlay :   curl https://.../api/v1/substitution?overlay=as4Kj  GET /api/v1/substitution?overlay=as4Kj  Result is a list of substituted compendia based on the given overlay compendium:  200 OK\n\n{\n  \"results\": [\n    \"9pQ34\",\n    \"1Tnd3\",\n    \u2026\n  ]\n}   Filter by  base  and  overlay :   curl https://.../api/v1/substitution?base=lO3Td&overlay=as4Kj  GET /api/v1/substitution?base=lO3Td&overlay=as4Kj  Result is a list of substituted compendia based on the given base  and  overlay compendium:  200 OK\n\n{\n  \"results\": [\n    \"9pQ34\",\n    \u2026\n  ]\n}",
            "title": "Filter results with following parameters:"
        },
        {
            "location": "/compendium/substitute/#error-responses_1",
            "text": "401 Unauthorized\n\n{\"error\":\"not authenticated\"}  401 Unauthorized\n\n{\"error\":\"not allowed\"}  404 Not Found\n\n{\"error\":\"base compendium not found\"}  404 Not Found\n\n{\"error\":\"overlay compendium not found\"}  400 Not Found\n\n{\"error\":\"base file is undefined\"}  400 Not Found\n\n{\"error\":\"overlay file is undefined\"}",
            "title": "Error responses"
        },
        {
            "location": "/compendium/substitute/#url-parameters-for-substituted-compendium-lists",
            "text": ":base  - id of the base compendium that the results should be related to  :overlay  - id of the overlay compendium that the results should be related to",
            "title": "URL parameters for substituted compendium lists"
        },
        {
            "location": "/job/",
            "text": "Execute a compendium\n\u00b6\n\n\nExecution jobs are used to run the analysis in a compendium.\nWhen a new execution job is started, the contents of the research compendium are cloned to create a trackable execution (see \nJob files\n).\nThe status information, logs and final working directory data are saved in their final state, so that they can be reviewed later on.\n\n\nAll execution jobs are tied to a single research compendium and reflect the execution history of that research compendium.\n\n\nA trivial execution job would be a completely unmodified compendium, to test the executability and thus basic reproducibility of the contained workflow.\n\n\nJob files\n\u00b6\n\n\nAll files except the following are copied to a separate storage for each job:\n\n\n\n\nexisting image tarballs, e.g. \nimage.tar\n to reduce size of copied files and because jobs use images from the local image repository anyway\n\n\nthe \ndisplay file\n to make sure the check does not wrongly work on the original display file\n\n\n\n\nJob status\n\u00b6\n\n\nThe property \njob.status\n shows the \noverall status\n of a job.\n\n\nThe overall status can be one of following:\n\n\n\n\nsuccess\n - if status of all steps is \nsuccess\n.\n\n\nfailure\n - if status of at least one step is \nfailure\n.\n\n\nrunning\n - if status of at least one step is \nrunning\n and no status is \nfailure\n.\n\n\n\n\nMore information about \nsteps\n can be found in subsection \nSteps\n of section \nView single job\n.\n\n\nSteps of a job\n\u00b6\n\n\nOne job consists of a series of steps.\nThe are executed in order.\n\n\n\n\nvalidate_bag\n\n  Validate the BagIt bag using the npm library \nbagit\n; may be skipped if compendium is not a bag, will usually fail because of added metadata files during upload.\n\n\ngenerate_configuration\n\n  Create a compendium configuration file; may be skipped if configuration file is already present.\n\n\nvalidate_compendium\n\n  Parses and validates the bagtainer configuration file.\n\n\ngenerate_manifest\n\n  Executes the given analysis to create a container manifest; may be skipped if manifest file is already present.\n\n\nimage_prepare\n\n  Create an archive of the payload (i.e. the workspace, or the data in a BagIt bag), which allows to build and run the image also on remote hosts.\n\n\nimage_build\n\n  Build an image and tag it \nerc:<job_id>\n.\n\n\nimage_execute\n\n  Run the container and return based on status code of program that ran inside the container.\n\n\ncheck\n\n  Run a check on the contents of the container. Validate the results of the executed calculations. The check provides either a list of errors or a reference to displayable content in the property \ndisplay.diff\n.\n\n\nimage_save\n\n  Export the image to a file within the compendium directory (potentially a large file!). This is skipped if the check failed.\n\n\ncleanup\n\n  Remove image or job files (depending on server-side settings).\n\n\n\n\nStatus\n\u00b6\n\n\nAll steps can have one of the following status:\n\n\n\n\nqueued\n: step is not yet started\n\n\nrunning\n: step is currently running\n\n\nsuccess\n: step is completed successfully - positive result\n\n\nfailure\n: step is completed unsuccessfully - negative result\n\n\nskipped\n: step does not fit the given input or results of previous steps, e.g. bag validation is not done for non-bag workspaces - neutral result\n\n\n\n\nStep metadata\n\u00b6\n\n\nAdditional explanations on steps' status will be transmitted in the \ntext\n property.\n\ntext\n is an array, with the latest element holding the newest information.\nDuring long running steps, the \ntext\n element is updated by appending new information when available.\n\n\nThe \nstart\n and \nend\n timestamps indicate the start and end time of the step. They are formatted as ISO8601.\n\n\nSpecific steps may carry more information in additional properties.\n\n\nNew job\n\u00b6\n\n\nCreate and run a new execution job with an HTTP \nPOST\n request using \nmultipart/form-data\n.\nRequires a \ncompendium_id\n.\n\n\n\n\nRequired user level and authentication\n\n\nThe user creating a new compendium must have the required \nuser level\n.\nRequests must be authenticated with a cookie \nconnect.sid\n, see \nuser authentication\n.\n\n\n\n\ncurl -F compendium_id=$ID https://\u2026/api/v1/job\n\n\nPOST /api/v1/job\n\n\n200 OK\n\n{\"job_id\":\"ngK4m\"}\n\n\n\n\nBody parameters for new jobs\n\u00b6\n\n\n\n\ncompendium_id\n (\nstring\n): \nRequired\n The identifier of the compendium to base this job on.\n\n\n\n\nError responses for new jobs\n\u00b6\n\n\n404 Not Found\n\n{\"error\":\"no compendium with this ID found\"}\n\n\n\n\n500 Internal Server Error\n\n{\"error\":\"could not create job\"}\n\n\n\n\nList jobs\n\u00b6\n\n\nLists jobs with filtering and pagination, returning up to 100 results by default.\n\n\nResults are be sorted by descending date of last change. The content of the response can be limited to certain properties of each result by providing a list of fields, i.e. the parameter \nfields\n.\n\n\nResults can be filtered:\n\n\n\n\nby \ncompendium_id\n i.e. \ncompendium_id=a4Dnm\n,\n\n\nby \nstatus\n i.e. \nstatus=success\n or\n\n\nby \nuser\n i.e. \nuser=0000-0000-0000-0001\n\n\n\n\ncurl https://\u2026/api/v1/job?limit=100&start=2&compendium_id=$ID&status=success&fields=status\n\n\nGET /api/v1/job?limit=100&start=2&compendium_id=a4Dnm&status=success\n\n\n200 OK\n\n{\n  \"results\": [\n    \"nkm4L\",\n    \"asdi5\",\n    \"nb2sg\",\n    \u2026\n  ]\n}\n\n\n\n\nThe overall job state can be added to the job list response:\n\n\nGET /api/v1/job?limit=100&start=2&compendium_id=a4Dnm&status=success&fields=status\n\n\n200 OK\n\n{\n  \"results\": [\n    {\n      \"id\":\"nkm4L\",\n      \"status\":\"failure\"\n    },\n    {\n      \"id\":\"asdi5\",\n      \"status\":\"success\"\n    },\n    {\n      \"id\":\"nb2sg\",\n      \"status\":\"running\"\n    },\n    \u2026\n  ]\n}\n\n\n\n\nIf there are no jobs, the returned list is empty:\n\n\n200 OK\n{\n  \"results\": [ ]\n}\n\n\n\n\nGET query parameters for listing jobs\n\u00b6\n\n\n\n\ncompendium_id\n - Comma-separated list of related compendium ids to filter by.\n\n\nstart\n - Starting point of the result list. \nstart - 1\n results are skipped. Defaults to 1.\n\n\nlimit\n - Limits the number of results in the response. Defaults to 100.\n\n\nstatus\n - Specify status to filter by. Can contain following \nstatus\n: \nsuccess\n, \nfailure\n, \nrunning\n.\n\n\nuser\n - Public user identifier to filter by.\n\n\nfields\n - Specify which additional attributes results list should contain. Can contain following fields: \nstatus\n, \nuser\n. Defaults to none.\n\n\n\n\nView single job\n\u00b6\n\n\nView details for a single job.\nThe file listing format is described in \ncompendium files\n.\n\n\ncurl https://\u2026/api/v1/job/$ID?steps=all\n\n\nGET /api/v1/job/:id?steps=all\n\n\n200 OK\n\n{\n  \"id\":\"UMmJ7\",\n  \"compendium_id\":\"BSgxj\",\n  \"steps\":{\n    \"validate_bag\":{\n      \"status\":\"skipped\",\n      \"text\":[\n        \"Not a bag\"\n      ],\n      \"end\":\"2017-11-17T13:22:48.105Z\",\n      \"start\":\"2017-11-17T13:22:48.105Z\"\n    },\n    \"generate_configuration\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"configuration file not found, generating it...\",\n        \"Saved configuration file to job and compendium\"\n      ],\n      \"end\":\"2017-11-17T13:22:48.119Z\",\n      \"start\":\"2017-11-17T13:22:48.113Z\"\n    },\n    \"validate_compendium\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"all checks passed\"\n      ],\n      \"end\":\"2017-11-17T13:22:48.127Z\",\n      \"start\":\"2017-11-17T13:22:48.125Z\"\n    },\n    \"generate_manifest\":{\n      \"status\":\"success\",\n      \"text\":[\n        /* abbreviated */\n        \"INFO [2017-11-17 13:22:56] Going online? TRUE  ... to retrieve system dependencies (sysreq-api)\",\n        \"INFO [2017-11-17 13:22:56] Trying to determine system requirements for the package(s) 'knitr, backports, magrittr, rprojroot, htmltools, yaml, Rcpp, stringi, rmarkdown, stringr, digest, evaluate' from sysreq online DB\",\n        \"INFO [2017-11-17 13:22:58] Adding CRAN packages: backports, digest, evaluate, htmltools, knitr, magrittr, Rcpp, rmarkdown, rprojroot, stringi, stringr, yaml\",\n        \"INFO [2017-11-17 13:22:58] Created Dockerfile-Object based on /erc/main.Rmd\",\n        \"INFO [2017-11-17 13:22:58] Writing dockerfile to /erc/Dockerfile\",\n        /* abbreviated */\n        \"generated manifest\"\n      ],\n      \"manifest\":\"Dockerfile\",\n      \"end\":\"2017-11-17T13:22:58.865Z\",\n      \"start\":\"2017-11-17T13:22:48.129Z\"\n    },\n    \"image_prepare\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"payload with 756224 total bytes created\"\n      ],\n      \"end\":\"2017-11-17T13:22:58.906Z\",\n      \"start\":\"2017-11-17T13:22:58.875Z\"\n    },\n    \"image_build\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"Step 1/6 : FROM rocker/r-ver:3.4.2\",\n        \"---> 3cf05960bf30\",\n        /* abbreviated */\n        \"---> Running in eb7ccd432592\",\n        \"---> 84db129215f6\",\n        \"Removing intermediate container eb7ccd432592\",\n        \"Successfully built 84db129215f6\",\n        \"Successfully tagged erc:UMmJ7\"\n      ],\n      \"end\":\"2017-11-17T13:22:59.899Z\",\n      \"start\":\"2017-11-17T13:22:58.912Z\"\n    },\n    \"image_execute\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"[started image execution]\",\n        /* abbreviated */\n        \"Output created: display.html\\r\\n> \\r\\n>\",\n        \"[finished image execution]\"\n      ],\n      \"statuscode\":0,\n      \"start\":\"2017-11-17T13:22:59.904Z\"\n    },\n    \"check\":{\n      \"status\":\"failure\",\n      \"text\":[\n        \"Check failed\"\n      ],\n      \"images\":[\n        {\n          \"imageIndex\":0,\n          \"resizeOperationCode\":0,\n          \"compareResults\":{\n            \"differences\":204786,\n            \"dimension\":1290240\n          }\n        }\n      ],\n      \"display\":{\n        \"diff\":\"/api/v1/job/UMmJ7/data/diffHTML.html\"\n      },\n      \"errors\":[ ],\n      \"checkSuccessful\":false,\n      \"end\":\"2017-11-17T13:23:04.439Z\",\n      \"start\":\"2017-11-17T13:23:03.479Z\"\n    },\n    \"image_save\": {\n      \"status\": \"success\",\n      \"text\": [\n        \"[Saving image tarball file]\",\n        \"[Saved image tarball to file (size: 875.14 MB)]\"\n      ],\n      \"start\": \"2018-01-29T17:38:55.111Z\",\n      \"file\": \"image.tar\",\n      \"end\": \"2018-01-29T17:39:36.845Z\"\n    },\n    \"cleanup\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"Running regular cleanup\",\n        \"Removed image with tag erc:UMmJ7: [{\\\"Untagged\\\":\\\"erc:UMmJ7\\\"},{\\\"Deleted\\\":\\\"sha256:84db129215f60f805320e0f70c54a706b6e4030f4627c74abfb1e17f287fefa8\\\"},{\\\"Deleted\\\":\\\"sha256:0dc5b951dc58a10e50ea42dd14a1cd59b080199d9ca40cadd0a4fc8ae5e0d139\\\"},{\\\"Deleted\\\":\\\"sha256:ea88669b92a1c67dc2825f9f6d90d334a6032882d3d31bc85671afbd04adaa70\\\"}]\",\n        \"Deleted temporary payload file.\"\n      ],\n      \"end\":\"2017-11-17T13:23:05.592Z\",\n      \"start\":\"2017-11-17T13:23:04.575Z\"\n    }\n  },\n  \"status\":\"failure\",\n  \"files\":{ /* see compendium */  }\n}\n\n\n\n\nURL parameters for single job view\n\u00b6\n\n\n\n\n:id\n - id of the job to be viewed\n\n\nsteps\n - Steps to be loaded with full details\n\n\n\n\nThe properties \nstatus\n, \nstart\n and \nend\n of \nall steps\n are always included in the response.\n\n\nSupported values for \nsteps\n are \nall\n or a comma separated list of one or more step names, e.g. \ngenerate_configuration,check\n.\nThe response will contain the default properties for all steps but other properties only for the selected ones.\nAny other values for \nsteps\n or not providing the parameter at all will return the default (e.g. \nsteps=no\n).\n\n\nError responses for single job view\n\u00b6\n\n\n404 Not Found\n\n{\"error\":\"no compendium with this ID found\"}\n\n\n\n\nJob status updates\n\u00b6\n\n\nYou can subscribe to real time status updates on jobs using \nWebSockets\n.\nThe implementation is based on \nsocket.io\n and using their client is recommended.\n\n\nThe job log is available at \nhttps://o2r.uni-muenster.de\n under the namespace \napi/v1/logs/job\n.\n\n\n# create a socket.io client:\nvar socket = io('https://o2r.uni-muenster.de/api/v1/logs/job');",
            "title": "Job"
        },
        {
            "location": "/job/#execute-a-compendium",
            "text": "Execution jobs are used to run the analysis in a compendium.\nWhen a new execution job is started, the contents of the research compendium are cloned to create a trackable execution (see  Job files ).\nThe status information, logs and final working directory data are saved in their final state, so that they can be reviewed later on.  All execution jobs are tied to a single research compendium and reflect the execution history of that research compendium.  A trivial execution job would be a completely unmodified compendium, to test the executability and thus basic reproducibility of the contained workflow.",
            "title": "Execute a compendium"
        },
        {
            "location": "/job/#job-files",
            "text": "All files except the following are copied to a separate storage for each job:   existing image tarballs, e.g.  image.tar  to reduce size of copied files and because jobs use images from the local image repository anyway  the  display file  to make sure the check does not wrongly work on the original display file",
            "title": "Job files"
        },
        {
            "location": "/job/#job-status",
            "text": "The property  job.status  shows the  overall status  of a job.  The overall status can be one of following:   success  - if status of all steps is  success .  failure  - if status of at least one step is  failure .  running  - if status of at least one step is  running  and no status is  failure .   More information about  steps  can be found in subsection  Steps  of section  View single job .",
            "title": "Job status"
        },
        {
            "location": "/job/#steps-of-a-job",
            "text": "One job consists of a series of steps.\nThe are executed in order.   validate_bag \n  Validate the BagIt bag using the npm library  bagit ; may be skipped if compendium is not a bag, will usually fail because of added metadata files during upload.  generate_configuration \n  Create a compendium configuration file; may be skipped if configuration file is already present.  validate_compendium \n  Parses and validates the bagtainer configuration file.  generate_manifest \n  Executes the given analysis to create a container manifest; may be skipped if manifest file is already present.  image_prepare \n  Create an archive of the payload (i.e. the workspace, or the data in a BagIt bag), which allows to build and run the image also on remote hosts.  image_build \n  Build an image and tag it  erc:<job_id> .  image_execute \n  Run the container and return based on status code of program that ran inside the container.  check \n  Run a check on the contents of the container. Validate the results of the executed calculations. The check provides either a list of errors or a reference to displayable content in the property  display.diff .  image_save \n  Export the image to a file within the compendium directory (potentially a large file!). This is skipped if the check failed.  cleanup \n  Remove image or job files (depending on server-side settings).",
            "title": "Steps of a job"
        },
        {
            "location": "/job/#status",
            "text": "All steps can have one of the following status:   queued : step is not yet started  running : step is currently running  success : step is completed successfully - positive result  failure : step is completed unsuccessfully - negative result  skipped : step does not fit the given input or results of previous steps, e.g. bag validation is not done for non-bag workspaces - neutral result",
            "title": "Status"
        },
        {
            "location": "/job/#step-metadata",
            "text": "Additional explanations on steps' status will be transmitted in the  text  property. text  is an array, with the latest element holding the newest information.\nDuring long running steps, the  text  element is updated by appending new information when available.  The  start  and  end  timestamps indicate the start and end time of the step. They are formatted as ISO8601.  Specific steps may carry more information in additional properties.",
            "title": "Step metadata"
        },
        {
            "location": "/job/#new-job",
            "text": "Create and run a new execution job with an HTTP  POST  request using  multipart/form-data .\nRequires a  compendium_id .   Required user level and authentication  The user creating a new compendium must have the required  user level .\nRequests must be authenticated with a cookie  connect.sid , see  user authentication .   curl -F compendium_id=$ID https://\u2026/api/v1/job  POST /api/v1/job  200 OK\n\n{\"job_id\":\"ngK4m\"}",
            "title": "New job"
        },
        {
            "location": "/job/#body-parameters-for-new-jobs",
            "text": "compendium_id  ( string ):  Required  The identifier of the compendium to base this job on.",
            "title": "Body parameters for new jobs"
        },
        {
            "location": "/job/#error-responses-for-new-jobs",
            "text": "404 Not Found\n\n{\"error\":\"no compendium with this ID found\"}  500 Internal Server Error\n\n{\"error\":\"could not create job\"}",
            "title": "Error responses for new jobs"
        },
        {
            "location": "/job/#list-jobs",
            "text": "Lists jobs with filtering and pagination, returning up to 100 results by default.  Results are be sorted by descending date of last change. The content of the response can be limited to certain properties of each result by providing a list of fields, i.e. the parameter  fields .  Results can be filtered:   by  compendium_id  i.e.  compendium_id=a4Dnm ,  by  status  i.e.  status=success  or  by  user  i.e.  user=0000-0000-0000-0001   curl https://\u2026/api/v1/job?limit=100&start=2&compendium_id=$ID&status=success&fields=status  GET /api/v1/job?limit=100&start=2&compendium_id=a4Dnm&status=success  200 OK\n\n{\n  \"results\": [\n    \"nkm4L\",\n    \"asdi5\",\n    \"nb2sg\",\n    \u2026\n  ]\n}  The overall job state can be added to the job list response:  GET /api/v1/job?limit=100&start=2&compendium_id=a4Dnm&status=success&fields=status  200 OK\n\n{\n  \"results\": [\n    {\n      \"id\":\"nkm4L\",\n      \"status\":\"failure\"\n    },\n    {\n      \"id\":\"asdi5\",\n      \"status\":\"success\"\n    },\n    {\n      \"id\":\"nb2sg\",\n      \"status\":\"running\"\n    },\n    \u2026\n  ]\n}  If there are no jobs, the returned list is empty:  200 OK\n{\n  \"results\": [ ]\n}",
            "title": "List jobs"
        },
        {
            "location": "/job/#get-query-parameters-for-listing-jobs",
            "text": "compendium_id  - Comma-separated list of related compendium ids to filter by.  start  - Starting point of the result list.  start - 1  results are skipped. Defaults to 1.  limit  - Limits the number of results in the response. Defaults to 100.  status  - Specify status to filter by. Can contain following  status :  success ,  failure ,  running .  user  - Public user identifier to filter by.  fields  - Specify which additional attributes results list should contain. Can contain following fields:  status ,  user . Defaults to none.",
            "title": "GET query parameters for listing jobs"
        },
        {
            "location": "/job/#view-single-job",
            "text": "View details for a single job.\nThe file listing format is described in  compendium files .  curl https://\u2026/api/v1/job/$ID?steps=all  GET /api/v1/job/:id?steps=all  200 OK\n\n{\n  \"id\":\"UMmJ7\",\n  \"compendium_id\":\"BSgxj\",\n  \"steps\":{\n    \"validate_bag\":{\n      \"status\":\"skipped\",\n      \"text\":[\n        \"Not a bag\"\n      ],\n      \"end\":\"2017-11-17T13:22:48.105Z\",\n      \"start\":\"2017-11-17T13:22:48.105Z\"\n    },\n    \"generate_configuration\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"configuration file not found, generating it...\",\n        \"Saved configuration file to job and compendium\"\n      ],\n      \"end\":\"2017-11-17T13:22:48.119Z\",\n      \"start\":\"2017-11-17T13:22:48.113Z\"\n    },\n    \"validate_compendium\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"all checks passed\"\n      ],\n      \"end\":\"2017-11-17T13:22:48.127Z\",\n      \"start\":\"2017-11-17T13:22:48.125Z\"\n    },\n    \"generate_manifest\":{\n      \"status\":\"success\",\n      \"text\":[\n        /* abbreviated */\n        \"INFO [2017-11-17 13:22:56] Going online? TRUE  ... to retrieve system dependencies (sysreq-api)\",\n        \"INFO [2017-11-17 13:22:56] Trying to determine system requirements for the package(s) 'knitr, backports, magrittr, rprojroot, htmltools, yaml, Rcpp, stringi, rmarkdown, stringr, digest, evaluate' from sysreq online DB\",\n        \"INFO [2017-11-17 13:22:58] Adding CRAN packages: backports, digest, evaluate, htmltools, knitr, magrittr, Rcpp, rmarkdown, rprojroot, stringi, stringr, yaml\",\n        \"INFO [2017-11-17 13:22:58] Created Dockerfile-Object based on /erc/main.Rmd\",\n        \"INFO [2017-11-17 13:22:58] Writing dockerfile to /erc/Dockerfile\",\n        /* abbreviated */\n        \"generated manifest\"\n      ],\n      \"manifest\":\"Dockerfile\",\n      \"end\":\"2017-11-17T13:22:58.865Z\",\n      \"start\":\"2017-11-17T13:22:48.129Z\"\n    },\n    \"image_prepare\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"payload with 756224 total bytes created\"\n      ],\n      \"end\":\"2017-11-17T13:22:58.906Z\",\n      \"start\":\"2017-11-17T13:22:58.875Z\"\n    },\n    \"image_build\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"Step 1/6 : FROM rocker/r-ver:3.4.2\",\n        \"---> 3cf05960bf30\",\n        /* abbreviated */\n        \"---> Running in eb7ccd432592\",\n        \"---> 84db129215f6\",\n        \"Removing intermediate container eb7ccd432592\",\n        \"Successfully built 84db129215f6\",\n        \"Successfully tagged erc:UMmJ7\"\n      ],\n      \"end\":\"2017-11-17T13:22:59.899Z\",\n      \"start\":\"2017-11-17T13:22:58.912Z\"\n    },\n    \"image_execute\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"[started image execution]\",\n        /* abbreviated */\n        \"Output created: display.html\\r\\n> \\r\\n>\",\n        \"[finished image execution]\"\n      ],\n      \"statuscode\":0,\n      \"start\":\"2017-11-17T13:22:59.904Z\"\n    },\n    \"check\":{\n      \"status\":\"failure\",\n      \"text\":[\n        \"Check failed\"\n      ],\n      \"images\":[\n        {\n          \"imageIndex\":0,\n          \"resizeOperationCode\":0,\n          \"compareResults\":{\n            \"differences\":204786,\n            \"dimension\":1290240\n          }\n        }\n      ],\n      \"display\":{\n        \"diff\":\"/api/v1/job/UMmJ7/data/diffHTML.html\"\n      },\n      \"errors\":[ ],\n      \"checkSuccessful\":false,\n      \"end\":\"2017-11-17T13:23:04.439Z\",\n      \"start\":\"2017-11-17T13:23:03.479Z\"\n    },\n    \"image_save\": {\n      \"status\": \"success\",\n      \"text\": [\n        \"[Saving image tarball file]\",\n        \"[Saved image tarball to file (size: 875.14 MB)]\"\n      ],\n      \"start\": \"2018-01-29T17:38:55.111Z\",\n      \"file\": \"image.tar\",\n      \"end\": \"2018-01-29T17:39:36.845Z\"\n    },\n    \"cleanup\":{\n      \"status\":\"success\",\n      \"text\":[\n        \"Running regular cleanup\",\n        \"Removed image with tag erc:UMmJ7: [{\\\"Untagged\\\":\\\"erc:UMmJ7\\\"},{\\\"Deleted\\\":\\\"sha256:84db129215f60f805320e0f70c54a706b6e4030f4627c74abfb1e17f287fefa8\\\"},{\\\"Deleted\\\":\\\"sha256:0dc5b951dc58a10e50ea42dd14a1cd59b080199d9ca40cadd0a4fc8ae5e0d139\\\"},{\\\"Deleted\\\":\\\"sha256:ea88669b92a1c67dc2825f9f6d90d334a6032882d3d31bc85671afbd04adaa70\\\"}]\",\n        \"Deleted temporary payload file.\"\n      ],\n      \"end\":\"2017-11-17T13:23:05.592Z\",\n      \"start\":\"2017-11-17T13:23:04.575Z\"\n    }\n  },\n  \"status\":\"failure\",\n  \"files\":{ /* see compendium */  }\n}",
            "title": "View single job"
        },
        {
            "location": "/job/#url-parameters-for-single-job-view",
            "text": ":id  - id of the job to be viewed  steps  - Steps to be loaded with full details   The properties  status ,  start  and  end  of  all steps  are always included in the response.  Supported values for  steps  are  all  or a comma separated list of one or more step names, e.g.  generate_configuration,check .\nThe response will contain the default properties for all steps but other properties only for the selected ones.\nAny other values for  steps  or not providing the parameter at all will return the default (e.g.  steps=no ).",
            "title": "URL parameters for single job view"
        },
        {
            "location": "/job/#error-responses-for-single-job-view",
            "text": "404 Not Found\n\n{\"error\":\"no compendium with this ID found\"}",
            "title": "Error responses for single job view"
        },
        {
            "location": "/job/#job-status-updates",
            "text": "You can subscribe to real time status updates on jobs using  WebSockets .\nThe implementation is based on  socket.io  and using their client is recommended.  The job log is available at  https://o2r.uni-muenster.de  under the namespace  api/v1/logs/job .  # create a socket.io client:\nvar socket = io('https://o2r.uni-muenster.de/api/v1/logs/job');",
            "title": "Job status updates"
        },
        {
            "location": "/search/",
            "text": "Search\n\u00b6\n\n\nThe search uses a document database to provide high speed and powerful search capabilities for compendia, including spatial and temporal properties.\n\n\nThe search structure is based on \nElasticsearch\n and thereby eases an implementation, because the requests and responses shown here can be directly mapped to respectively from \nElasticsearch's API\n.\n\n\nIndexed information:\n\n\n\n\ncompendium \nmetadata\n (including harvested and user-edited metadata such as temporal ranges and spatial extents)\n\n\nfull texts\n of text files in a compendium\n\n\n\n\nSimple search\n\u00b6\n\n\nA simple search allows searching for search terms using an \nHTTP GET\n request accepting \napplication/json\n content type.\n\n\ncurl -H 'Content-Type: application/json' https://.../api/v1/search?q=$SEARCHTERM\n\n\nGET /api/v1/search?q=Reproducible\n\n\nGET /api/v1/search?q=great reproducible research\n\n\nThe \nresponse\n is \nJSON\n with the root element is \nhits\n, which has the same as the \nhits\n element from an Elasticsearch response but does not include internal fields such as \n_index\n, \n_type\n, and \n_id\n.\n\n\n200 ok\n\n{\n  \"hits\": {\n    \"total\": 1,\n    \"max_score\": 1.0586987,\n    \"hits\": [\n      {\n        \"_score\": 1.0586987,\n        \"_source\": {\n          \"metadata\": {\n            \"o2r\": ...\n          },\n        }\n      }\n    ]\n  }\n}\n\n\n\n\n\n\nNote\n\n\nThe available metadata is a synced clone of the compendium metadata stored in the main database.\nFor more information on the mapping from the main database to the search database, take a look at the \no2r-finder\n microservice\n.\n\n\n\n\nQuery parameters for simple search\n\u00b6\n\n\n\n\nq\n - search term(s), must be \nURL-encoded\n\n\nresources\n - a comma-separated list of resources to include in the search; supported values are\n\n\ncompendia\n\n\njobs\n\n\nall\n (default)\n\n\n\n\nExample requests\n\u00b6\n\n\n\n\nhttp://o2r.uni-muenster.de/api/v1/search?q=*\n\n\nhttp://o2r.uni-muenster.de/api/v1/search?q=europe temperature data analysis\n\n\nhttp://o2r.uni-muenster.de/api/v1/search?q=europe%20temperature%20data%20analysis\n\n\nhttp://o2r.uni-muenster.de/api/v1/search?q=10.5555%2F12345678\n\n\nhttp://o2r.uni-muenster.de/api/v1/search?q=geo&resources=compendia\n\n\nhttp://o2r.uni-muenster.de/api/v1/search?q=failure&resources=jobs\n\n\n\n\nComplex Search\n\u00b6\n\n\nA complex search is enabled via \nPOST\n requests with a \nJSON\n payload as \nHTTP POST\n data (\nnot\n \nmultipart/form-data\n) accepting an \napplication/json\n content type as response.\nQueries can include filters, aggregation and spatio-temporal operations as defined in the \nElasticsearch Query DSL\n.\n\n\ncurl -X POST -H 'Content-Type: application/json' 'https://.../api/v1/search' -d '$QUERY_DSL'\n\n\nThe \nresponse\n structure is the same as for \nsimple search\n.\n\n\n\n\nNote\n\n\nUse the index names \ncompendia\n and \njobs\n in a terms query to only retrieve one resource type.\n\n\n\n\nQuery fields  for complex search\n\u00b6\n\n\nThe following fields are especially relevant to build queries.\n\n\n\n\nmetadata.o2r.temporal.begin\n and \nmetadata.o2r.temporal.end\n provide a compendium's temporal extent\n\n\nmetadata.o2r.spatial.geometry\n has the compendium's spatial extent\n\n\n\n\nBesides these fields, all metadata of the \no2r\n metadata format\n can be used.\n\n\nExamples\n\u00b6\n\n\nTemporal search\n\u00b6\n\n\nPOST /api/v1/search -d '{\n  \"query\": {\n      \"bool\": {\n          \"must\": {\n              \"match_all\": {}\n          },\n          \"filter\": [\n              {\n                  \"range\": {\n                      \"metadata.o2r.temporal.begin\": {\n                          \"from\": \"2015-03-01T00:00:00.000Z\"\n                      }\n                  }\n              },\n              {\n                  \"range\": {\n                      \"metadata.o2r.temporal.end\": {\n                          \"to\": \"2017-10-01T00:00:00.000Z\"\n                      }\n                  }\n              }\n          ]\n      }\n  },\n  \"from\": 0,\n  \"size\": 10\n}'\n\n\n\n\nSpatial search\n\u00b6\n\n\n{\n    \"bool\": {\n        \"must\": {\n            \"match_all\": {}\n         },\n         \"filter\": {\n              \"geo_shape\": {\n                   \"metadata.o2r.spatial.geometry\": {\n                        \"shape\": {\n                            \"type\": \"polygon\",\n                            \"coordinates\": [... GeoJSON coordinates...]\n                         },\n                         \"relation\": \"within\"\n                    }\n               }\n          }\n     }\n}\n\n\n\n\nIn this example a filter has been nested within a \nboolean/must match\n query.\nThe filter has been applied to the \nmetadata.o2r.spatial.geometry\n field of the dataset with a \nwithin\n relation so that only compendia with a spatial extent completely contained in the provided shape are fetched.\n\n\nResource search\n\u00b6\n\n\n{\n    \"query\": {\n        \"terms\": {\n            \"_index\": [\"compendia\"]\n        }\n    }\n}\n\n\n\n\nResponse\n\u00b6\n\n\n200 ok\n\n{\n  \"hits\": {\n    \"total\": 1,\n    \"max_score\": 1.0586987,\n    \"hits\": [\n      {\n        \"_score\": 1.0586987,\n        \"_source\": {\n          \"metadata\": {\n            \"o2r\": ...\n          },\n        }\n      }\n    ]\n  }\n}",
            "title": "Search"
        },
        {
            "location": "/search/#search",
            "text": "The search uses a document database to provide high speed and powerful search capabilities for compendia, including spatial and temporal properties.  The search structure is based on  Elasticsearch  and thereby eases an implementation, because the requests and responses shown here can be directly mapped to respectively from  Elasticsearch's API .  Indexed information:   compendium  metadata  (including harvested and user-edited metadata such as temporal ranges and spatial extents)  full texts  of text files in a compendium",
            "title": "Search"
        },
        {
            "location": "/search/#simple-search",
            "text": "A simple search allows searching for search terms using an  HTTP GET  request accepting  application/json  content type.  curl -H 'Content-Type: application/json' https://.../api/v1/search?q=$SEARCHTERM  GET /api/v1/search?q=Reproducible  GET /api/v1/search?q=great reproducible research  The  response  is  JSON  with the root element is  hits , which has the same as the  hits  element from an Elasticsearch response but does not include internal fields such as  _index ,  _type , and  _id .  200 ok\n\n{\n  \"hits\": {\n    \"total\": 1,\n    \"max_score\": 1.0586987,\n    \"hits\": [\n      {\n        \"_score\": 1.0586987,\n        \"_source\": {\n          \"metadata\": {\n            \"o2r\": ...\n          },\n        }\n      }\n    ]\n  }\n}   Note  The available metadata is a synced clone of the compendium metadata stored in the main database.\nFor more information on the mapping from the main database to the search database, take a look at the  o2r-finder  microservice .",
            "title": "Simple search"
        },
        {
            "location": "/search/#query-parameters-for-simple-search",
            "text": "q  - search term(s), must be  URL-encoded  resources  - a comma-separated list of resources to include in the search; supported values are  compendia  jobs  all  (default)",
            "title": "Query parameters for simple search"
        },
        {
            "location": "/search/#example-requests",
            "text": "http://o2r.uni-muenster.de/api/v1/search?q=*  http://o2r.uni-muenster.de/api/v1/search?q=europe temperature data analysis  http://o2r.uni-muenster.de/api/v1/search?q=europe%20temperature%20data%20analysis  http://o2r.uni-muenster.de/api/v1/search?q=10.5555%2F12345678  http://o2r.uni-muenster.de/api/v1/search?q=geo&resources=compendia  http://o2r.uni-muenster.de/api/v1/search?q=failure&resources=jobs",
            "title": "Example requests"
        },
        {
            "location": "/search/#complex-search",
            "text": "A complex search is enabled via  POST  requests with a  JSON  payload as  HTTP POST  data ( not   multipart/form-data ) accepting an  application/json  content type as response.\nQueries can include filters, aggregation and spatio-temporal operations as defined in the  Elasticsearch Query DSL .  curl -X POST -H 'Content-Type: application/json' 'https://.../api/v1/search' -d '$QUERY_DSL'  The  response  structure is the same as for  simple search .   Note  Use the index names  compendia  and  jobs  in a terms query to only retrieve one resource type.",
            "title": "Complex Search"
        },
        {
            "location": "/search/#query-fields-for-complex-search",
            "text": "The following fields are especially relevant to build queries.   metadata.o2r.temporal.begin  and  metadata.o2r.temporal.end  provide a compendium's temporal extent  metadata.o2r.spatial.geometry  has the compendium's spatial extent   Besides these fields, all metadata of the  o2r  metadata format  can be used.",
            "title": "Query fields  for complex search"
        },
        {
            "location": "/search/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/search/#temporal-search",
            "text": "POST /api/v1/search -d '{\n  \"query\": {\n      \"bool\": {\n          \"must\": {\n              \"match_all\": {}\n          },\n          \"filter\": [\n              {\n                  \"range\": {\n                      \"metadata.o2r.temporal.begin\": {\n                          \"from\": \"2015-03-01T00:00:00.000Z\"\n                      }\n                  }\n              },\n              {\n                  \"range\": {\n                      \"metadata.o2r.temporal.end\": {\n                          \"to\": \"2017-10-01T00:00:00.000Z\"\n                      }\n                  }\n              }\n          ]\n      }\n  },\n  \"from\": 0,\n  \"size\": 10\n}'",
            "title": "Temporal search"
        },
        {
            "location": "/search/#spatial-search",
            "text": "{\n    \"bool\": {\n        \"must\": {\n            \"match_all\": {}\n         },\n         \"filter\": {\n              \"geo_shape\": {\n                   \"metadata.o2r.spatial.geometry\": {\n                        \"shape\": {\n                            \"type\": \"polygon\",\n                            \"coordinates\": [... GeoJSON coordinates...]\n                         },\n                         \"relation\": \"within\"\n                    }\n               }\n          }\n     }\n}  In this example a filter has been nested within a  boolean/must match  query.\nThe filter has been applied to the  metadata.o2r.spatial.geometry  field of the dataset with a  within  relation so that only compendia with a spatial extent completely contained in the provided shape are fetched.",
            "title": "Spatial search"
        },
        {
            "location": "/search/#resource-search",
            "text": "{\n    \"query\": {\n        \"terms\": {\n            \"_index\": [\"compendia\"]\n        }\n    }\n}",
            "title": "Resource search"
        },
        {
            "location": "/search/#response",
            "text": "200 ok\n\n{\n  \"hits\": {\n    \"total\": 1,\n    \"max_score\": 1.0586987,\n    \"hits\": [\n      {\n        \"_score\": 1.0586987,\n        \"_source\": {\n          \"metadata\": {\n            \"o2r\": ...\n          },\n        }\n      }\n    ]\n  }\n}",
            "title": "Response"
        },
        {
            "location": "/shipment/",
            "text": "Ship compendia and metadata\n\u00b6\n\n\nShipments are used to deliver compendia and their metadata to third party repositories or archives.\nThis section covers shipment related requests, including repository file management.\n\n\nPackaging\n\u00b6\n\n\nThe packaging of a compendium ensures a recipient can verify the integrity of the transported data.\nCurrently, the shipment process always creates \nBagIt\n bags to package a compendium.\n\n\nSupported recipients\n\u00b6\n\n\nUse the \nrecipient\n endpoint to find out, which repositories are available and configured.\nThe response is list of tuples with \nid\n and \nlabel\n of each repository.\nThe \nid\n is the repository identifier to be used in requests to the \n/shipment\n endpoint, e.g. to define the recipient, while \nlabel\n is a human-readable text string suitable for display in user interfaces.\n\n\nGET /api/v1/recipient\n\n\n200\n\n{\n    \"recipients\": [{\n        \"id\": \"download\",\n        \"label\": \"Download\"\n    }, {\n        \"id\": \"b2share_sandbox\",\n        \"label\": \"Eudat b2share Sandbox\"\n    }, {\n        \"id\": \"zenodo_sandbox\",\n        \"label\": \"Zenodo Sandbox\"\n    }]\n}\n\n\n\n\nAn implementation may support one or more of the following repositories:\n\n\n\n\nb2share\n - \nEudat b2share\n\n\nb2share_sandbox\n - \nEudat b2share Sandbox\n\n\nzenodo\n - \nZenodo Sandbox\n\n\nzenodo_sandbox\n - \nZenodo Sandbox\n\n\n\n\nThe \ndownload\n recipient is a surrogate to enable shipping to the user's local storage.\n\n\nList shipments\n\u00b6\n\n\nThis is a basic request to list all shipment identifiers.\n\n\nGET /api/v1/shipment\n\n\n200\n\n[\"dc351fc6-314f-4947-a235-734ab5971eff\", \"...\"]\n\n\n\n\n\nYou can also get only the shipment identifiers belonging to a compendium id (e.g. \n4XgD97\n).\n\n\nGET /api/v1/shipment?compendium_id=4XgD97\n\n\nURL parameter:\n\n\n\n\ncompendium_id\n - the identifier of a specific compendium\n\n\n\n\n200 \n\n[\"dc351fc6-314f-4947-a235-734ab5971eff\", \"...\"]\n\n\n\n\n\nGet a single shipment\n\u00b6\n\n\nGET /api/v1/shipment/dc351fc6-314f-4947-a235-734ab5971eff\n\n\n200 \n\n{\n  \"last_modified\": \"2016-12-12 10:34:32.001475\",\n  \"recipient\": \"zenodo\",\n  \"id\": \"dc351fc6-314f-4947-a235-734ab5971eff\",\n  \"deposition_id\": \"63179\",\n  \"user\": \"0000-0002-1825-0097\",\n  \"status\": \"shipped\",\n  \"compendium_id\": \"4XgD97\",\n  \"deposition_url\": \"https://zenodo.org/record/63179\"\n}\n\n\n\n\n\n\nNote\n\n\nReturned deposition URLs (property \ndeposition_url\n) from Zenodo as well as Eudat b2share (records) will only be functional after publishing.\n\n\n\n\nCreate a new shipment\n\u00b6\n\n\nYou can start a initial creation of a shipment, leading to transmission to a repository and creation of a deposition, using a \nPOST\n request.\n\n\nPOST /api/v1/shipment\n\n\nThis \nrequires\n the following parameters as \nmultipart/form-data\n or \napplication/x-www-form-urlencoded\n encoded data:\n\n\n\n\ncompendium_id\n (\nstring\n): the id of the compendium\n\n\nrecipient\n (\nstring\n): identifier for the repository\n\n\n\n\nThe following are \noptional parameters\n:\n\n\n\n\nupdate_packaging\n (\nboolean\n, default: \nfalse\n): the shipment creation only succeeds if a valid package is already present under the provided compendium identifier, or if no packaging is present at all and a new package can be created. In case a partial or invalid package is given, this parameter can control the shipment creation process: If it is set to \ntrue\n, the shipment package is updated during the shipment creation in order to make it valid, if set to \nfalse\n the shipment creation results in an error.\n\n\ncookie\n (\nstring\n): an authentication cookie must be set in the request header, but it may also be provided via a \ncookie\n form parameter as a fallback\n\n\nshipment_id\n (\nstring\n): a user-defined identifier for the shipment (see \nid\n in response)\n\n\n\n\n\n\nRequired user level\n\n\nThe user sending the request to create a shipment must have the required \nuser level\n.\n\n\n\n\nCreation response\n\u00b6\n\n\nThe response contains the shipment document, see \nGet a single shipment\n.\nSome of the fields are not available (have value \nnull\n) until after \npublishing\n, e.g. \ndeposition_url\n.\n\n\n201\n\n{\n  \"id\": \"9ff3d75e-23dc-423e-a6c6-6987ac5ffc3e\",\n  \"recipient\": \"zenodo\",\n  \"status\": \"shipped\",\n  \"deposition_id\": \"79102\"\n}\n\n\n\n\nIf the recipient is the \ndownload\n surrogate, the response will be \n202\n and a zip stream with the Content type \napplication/zip\n.\n\n\n202\n\n\n\n\n(zip stream starting point)\n\n\nThe download zip stream is also available under the url of the shipment plus \n/dl\n, once it has been created, e.g.:  \n\n\nhttp://localhost:8087/api/v1/shipment/22e7b17c-0047-4cb9-9041-bb87f30de388/dl\n\n\n\n\nShipment status\n\u00b6\n\n\nA shipment can have three possible status:\n\n\n\n\nshipped\n - a deposition has been created at a repository and completed the necessary metadata for publication.\n\n\npublished\n - the contents of the shipment are published on the repository, in which case the publishment can not be undone.\n\n\nerror\n - an error occurred during shipment or publishing.\n\n\n\n\nTo get only a shipment's current status you may use the sub-resource \n/status\n:\n\n\nGET api/v1/shipment/<shipment_id>/status\n\n\n200\n\n{\n\"id\": \"9ff3d75e-23dc-423e-a6c6-6987ac5ffc3e\",\n\"status\": \"shipped\"\n}\n\n\n\n\nPublish a deposition\n\u00b6\n\n\nThe publishment is supposed to have completed the status \nshipped\n where metadata requirements for publication have been checked.\n\n\n\n\nNote\n\n\nOnce published, a deposition can no longer be deleted on the supported repositories.\n\n\n\n\nPUT api/v1/shipment/<shipment_id>/publishment\n\n\n200\n\n{\n\"id\": \"9ff3d75e-23dc-423e-a6c6-6987ac5ffc3e\",\n\"status\": \"published\"\n}\n\n\n\n\nNote that a publishment is not possible if the recipient is the download surrogate which immediately results in a zip stream as a response.\n\n\nFiles in a deposition\n\u00b6\n\n\nList deposition files\n\u00b6\n\n\nYou can request a list of all files in a deposition and their properties with the sub-resource \n/publishment\n.\n\n\nGET api/v1/shipment/<shipment_id>/publishment\n\n\n200\n\n{\n\"files\": [{\n    \"filesize\": 393320,\n    \"id\": \"bae2a60c-bd59-47e1-a443-b34bb7d0a981\",\n    \"filename\": \"4XgD9.zip\",\n    \"checksum\": \"702f4db3e53b22176d1d5ddcda462a27\",\n    \"links\": {\n        \"self\": \"https://sandbox.zenodo.org/api/deposit/depositions/71552/files/bae2a60c-bd59-47e1-a443-b34bb7d0a981\",\n        \"download\": \"https://sandbox.zenodo.org/api/files/31dc8f3d-df00-4d8a-bd99-64ef341372b3/4XgD9.zip\"\n    }\n}]\n}\n\n\n\n\nYou can find the \nid\n of the file you want to interact with in this json list object at \nfiles[n].id\n, where \nn\n is the position of that file in the array.\nFiles can be identified in this response by either their id in the depot, their filename or their checksum.\n\n\nDelete a specific file from a deposition\n\u00b6\n\n\nYou can delete files from a \nshipped\n shipment's deposition.\nYou must state a file's identifier, which can be retrieved from the shipment's deposition files property \nid\n, as the \nfile_id\n path parameter.\nFiles for a \npublished\n shipment usually cannot be deleted.\n\n\nDELETE api/v1/shipment/<shipment_id>/files/<file_id>\n\n\n204\n\n\n\n\nError responses\n\u00b6\n\n\n400\n\n{\"error\":\"bad request\"}\n\n\n\n\n403\n\n{\"error\": \"insufficient permissions\"}",
            "title": "Shipment"
        },
        {
            "location": "/shipment/#ship-compendia-and-metadata",
            "text": "Shipments are used to deliver compendia and their metadata to third party repositories or archives.\nThis section covers shipment related requests, including repository file management.",
            "title": "Ship compendia and metadata"
        },
        {
            "location": "/shipment/#packaging",
            "text": "The packaging of a compendium ensures a recipient can verify the integrity of the transported data.\nCurrently, the shipment process always creates  BagIt  bags to package a compendium.",
            "title": "Packaging"
        },
        {
            "location": "/shipment/#supported-recipients",
            "text": "Use the  recipient  endpoint to find out, which repositories are available and configured.\nThe response is list of tuples with  id  and  label  of each repository.\nThe  id  is the repository identifier to be used in requests to the  /shipment  endpoint, e.g. to define the recipient, while  label  is a human-readable text string suitable for display in user interfaces.  GET /api/v1/recipient  200\n\n{\n    \"recipients\": [{\n        \"id\": \"download\",\n        \"label\": \"Download\"\n    }, {\n        \"id\": \"b2share_sandbox\",\n        \"label\": \"Eudat b2share Sandbox\"\n    }, {\n        \"id\": \"zenodo_sandbox\",\n        \"label\": \"Zenodo Sandbox\"\n    }]\n}  An implementation may support one or more of the following repositories:   b2share  -  Eudat b2share  b2share_sandbox  -  Eudat b2share Sandbox  zenodo  -  Zenodo Sandbox  zenodo_sandbox  -  Zenodo Sandbox   The  download  recipient is a surrogate to enable shipping to the user's local storage.",
            "title": "Supported recipients"
        },
        {
            "location": "/shipment/#list-shipments",
            "text": "This is a basic request to list all shipment identifiers.  GET /api/v1/shipment  200\n\n[\"dc351fc6-314f-4947-a235-734ab5971eff\", \"...\"]  You can also get only the shipment identifiers belonging to a compendium id (e.g.  4XgD97 ).  GET /api/v1/shipment?compendium_id=4XgD97  URL parameter:   compendium_id  - the identifier of a specific compendium   200 \n\n[\"dc351fc6-314f-4947-a235-734ab5971eff\", \"...\"]",
            "title": "List shipments"
        },
        {
            "location": "/shipment/#get-a-single-shipment",
            "text": "GET /api/v1/shipment/dc351fc6-314f-4947-a235-734ab5971eff  200 \n\n{\n  \"last_modified\": \"2016-12-12 10:34:32.001475\",\n  \"recipient\": \"zenodo\",\n  \"id\": \"dc351fc6-314f-4947-a235-734ab5971eff\",\n  \"deposition_id\": \"63179\",\n  \"user\": \"0000-0002-1825-0097\",\n  \"status\": \"shipped\",\n  \"compendium_id\": \"4XgD97\",\n  \"deposition_url\": \"https://zenodo.org/record/63179\"\n}   Note  Returned deposition URLs (property  deposition_url ) from Zenodo as well as Eudat b2share (records) will only be functional after publishing.",
            "title": "Get a single shipment"
        },
        {
            "location": "/shipment/#create-a-new-shipment",
            "text": "You can start a initial creation of a shipment, leading to transmission to a repository and creation of a deposition, using a  POST  request.  POST /api/v1/shipment  This  requires  the following parameters as  multipart/form-data  or  application/x-www-form-urlencoded  encoded data:   compendium_id  ( string ): the id of the compendium  recipient  ( string ): identifier for the repository   The following are  optional parameters :   update_packaging  ( boolean , default:  false ): the shipment creation only succeeds if a valid package is already present under the provided compendium identifier, or if no packaging is present at all and a new package can be created. In case a partial or invalid package is given, this parameter can control the shipment creation process: If it is set to  true , the shipment package is updated during the shipment creation in order to make it valid, if set to  false  the shipment creation results in an error.  cookie  ( string ): an authentication cookie must be set in the request header, but it may also be provided via a  cookie  form parameter as a fallback  shipment_id  ( string ): a user-defined identifier for the shipment (see  id  in response)    Required user level  The user sending the request to create a shipment must have the required  user level .",
            "title": "Create a new shipment"
        },
        {
            "location": "/shipment/#creation-response",
            "text": "The response contains the shipment document, see  Get a single shipment .\nSome of the fields are not available (have value  null ) until after  publishing , e.g.  deposition_url .  201\n\n{\n  \"id\": \"9ff3d75e-23dc-423e-a6c6-6987ac5ffc3e\",\n  \"recipient\": \"zenodo\",\n  \"status\": \"shipped\",\n  \"deposition_id\": \"79102\"\n}  If the recipient is the  download  surrogate, the response will be  202  and a zip stream with the Content type  application/zip .  202  (zip stream starting point)  The download zip stream is also available under the url of the shipment plus  /dl , once it has been created, e.g.:    http://localhost:8087/api/v1/shipment/22e7b17c-0047-4cb9-9041-bb87f30de388/dl",
            "title": "Creation response"
        },
        {
            "location": "/shipment/#shipment-status",
            "text": "A shipment can have three possible status:   shipped  - a deposition has been created at a repository and completed the necessary metadata for publication.  published  - the contents of the shipment are published on the repository, in which case the publishment can not be undone.  error  - an error occurred during shipment or publishing.   To get only a shipment's current status you may use the sub-resource  /status :  GET api/v1/shipment/<shipment_id>/status  200\n\n{\n\"id\": \"9ff3d75e-23dc-423e-a6c6-6987ac5ffc3e\",\n\"status\": \"shipped\"\n}",
            "title": "Shipment status"
        },
        {
            "location": "/shipment/#publish-a-deposition",
            "text": "The publishment is supposed to have completed the status  shipped  where metadata requirements for publication have been checked.   Note  Once published, a deposition can no longer be deleted on the supported repositories.   PUT api/v1/shipment/<shipment_id>/publishment  200\n\n{\n\"id\": \"9ff3d75e-23dc-423e-a6c6-6987ac5ffc3e\",\n\"status\": \"published\"\n}  Note that a publishment is not possible if the recipient is the download surrogate which immediately results in a zip stream as a response.",
            "title": "Publish a deposition"
        },
        {
            "location": "/shipment/#files-in-a-deposition",
            "text": "",
            "title": "Files in a deposition"
        },
        {
            "location": "/shipment/#list-deposition-files",
            "text": "You can request a list of all files in a deposition and their properties with the sub-resource  /publishment .  GET api/v1/shipment/<shipment_id>/publishment  200\n\n{\n\"files\": [{\n    \"filesize\": 393320,\n    \"id\": \"bae2a60c-bd59-47e1-a443-b34bb7d0a981\",\n    \"filename\": \"4XgD9.zip\",\n    \"checksum\": \"702f4db3e53b22176d1d5ddcda462a27\",\n    \"links\": {\n        \"self\": \"https://sandbox.zenodo.org/api/deposit/depositions/71552/files/bae2a60c-bd59-47e1-a443-b34bb7d0a981\",\n        \"download\": \"https://sandbox.zenodo.org/api/files/31dc8f3d-df00-4d8a-bd99-64ef341372b3/4XgD9.zip\"\n    }\n}]\n}  You can find the  id  of the file you want to interact with in this json list object at  files[n].id , where  n  is the position of that file in the array.\nFiles can be identified in this response by either their id in the depot, their filename or their checksum.",
            "title": "List deposition files"
        },
        {
            "location": "/shipment/#delete-a-specific-file-from-a-deposition",
            "text": "You can delete files from a  shipped  shipment's deposition.\nYou must state a file's identifier, which can be retrieved from the shipment's deposition files property  id , as the  file_id  path parameter.\nFiles for a  published  shipment usually cannot be deleted.  DELETE api/v1/shipment/<shipment_id>/files/<file_id>  204",
            "title": "Delete a specific file from a deposition"
        },
        {
            "location": "/shipment/#error-responses",
            "text": "400\n\n{\"error\":\"bad request\"}  403\n\n{\"error\": \"insufficient permissions\"}",
            "title": "Error responses"
        },
        {
            "location": "/user/",
            "text": "User\n\u00b6\n\n\nList users\n\u00b6\n\n\nReturn a list of user ids. \nPagination (including defaults) as described for compendia\n is available for users.\n\n\ncurl https://\u2026/api/v1/user\n\n\nGET /api/v1/user\n\n\n200 OK\n\n{\n    \"results\": [\n        \"0000-0002-1825-0097\",\n        \"0000-0002-1825-0097\"\n    ]\n}\n\n\n\n\nIf there are no users, the returned list is empty:\n\n\n200 OK\n{\n  \"results\": [ ]\n}\n\n\n\n\nPagination is supported using the query parameters \nstart\n and \nlimit\n.\n\n\n\n\nlimit\n is the number of results in the response, defaults to \n10\n. It numeric and larger than \n0\n.\n\n\nstart\n is the index of the first list item in the response, defaults to \n1\n. It must be numeric and larger than \n0\n.\n\n\n\n\nGET /api/v1/user?start=5&limit=10\n\n\nError responses for user list\n\u00b6\n\n\n400 Bad Request\n\n{\"error\":\"limit must be larger than 0\"}\n\n\n\n\nView single user\n\u00b6\n\n\nShow the details of a user.\n\n\ncurl https://\u2026/api/v1/user/$ID\n\n\nGET /api/v1/user/:id\n\n\n200 OK\n\n{\n    \"id\": \"0000-0002-1825-0097\",\n    \"name\": \"o2r\"\n}\n\n\n\n\nThe content of the response depends on the state and level of the user that requests the resource. The above response only contains the id and the publicly visible name. The following response contains more details and requires a certain user level of the authenticated user making the request:\n\n\ncurl --cookie \"connect.sid=<session cookie here>\" https://\u2026/api/v1/user/0000-0002-1825-0097\n\n\n200 OK\n\n{\n    \"id\": \"0000-0002-1825-0097\",\n    \"name\": \"o2r\",\n    \"level\": 0,\n    \"lastseen\": \"2016-08-15T12:32:23.972Z\"\n}\n\n\n\n\nURL parameters for single user view\n\u00b6\n\n\n\n\n:id\n - the user id\n\n\n\n\nError responses for single user view\n\u00b6\n\n\n404 Not Found\n\n{\"error\":\"no user with this id\"}\n\n\n\n\nAuthentication\n\u00b6\n\n\nUser authentication is done via authenticated sessions, which are referenced with a cookie called \nconnect.sid\n. For every endpoint that needs user authentication, a cookie with an authenticated session is required.\n\n\nClient authentication\n\u00b6\n\n\nTo execute restricted operations of the API, such as \ncompendium upload\n or \njob execution\n, a client must provide an authentication token via a cookie.\n\n\nA client must first login on the website to access a browser cookie issued by \no2r.uni-muenster.de\n with the name \nconnect.sid\n.\nProvide the content of the cookie when making requests to the API as shown in the request example below.\n\n\nAccess authentication information for direct API access\n\u00b6\n\n\nTo run commands which require authentication from the command line, a user must login on the website first.\nThen open you browser cookies and find a cookie issued by \no2r.uni-muenster.de\n with the name \nconnect.sid\n.\nUse the the contents of the cookie for your requests, for example as shown below when using curl.\n\n\ncurl [...] --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/endpoint\n\n\n\n\nAuthentication within microservices\n\u00b6\n\n\nAttention:\n The authentication process \nrequires\n a secured connection, i.e. \nHTTPS\n.\n\n\nAuthentication provider\n\u00b6\n\n\nSession authentication is done using the OAuth 2.0 protocol.\nCurrently \nORCID\n is the only available authentication provider, therefore users need to be registered with ORCID. Because of its nature, the authentication workflow is not a RESTful service.\nUsers must follow the redirection to the login endpoint with their web browser and grant access to the o2r reproducibility service for their ORCID account.\nThey are then sent back to our authentication service, which verifies the authentication request and enriches the user session with the verified ORCID for this user.\n\n\nStart OAuth login\n\u00b6\n\n\nNavigate the web browser (e.g. via a HTML \n<a>\n link) to \n/api/v1/auth/login\n, which then redirects the user and request access to your ORCID profile. After granting access, ORCID redirects the user back to the \n/api/v1/auth/login\n endpoint with a unique \ncode\n param that is used to verify the request.\n\n\nIf the verification was successful, the endpoint returns a session cookie named \nconnect.sid\n, which is tied to a authenticated session. The server answers with a \n301 redirect\n, which redirects the user back to \n/\n, the start page of the o2r website.\n\n\nIf the login is unsuccessful, the user is not redirected back to the site and no further redirects are configured.\n\n\nRequest authentication status\n\u00b6\n\n\nAs the cookie is present in both authenticated and unauthenticated sessions, clients (e.g. web browser user interfaces) must know if their session is authenticated, and if so, as which ORCID user. For this, send a \nGET\n request to the \n/api/v1/auth/whoami\n endpoint, including your session cookie.\n\n\ncurl https://\u2026/api/v1/auth/whoami --cookie \"connect.sid=\u2026\n\n\nGET /api/v1/auth/whoami\n\n\n200 OK\n\n{\n  \"orcid\": \"0000-0002-1825-0097\",\n  \"name\": \"o2r\"\n}\n\n\n\n\nError response for requests requiring authentication\n\u00b6\n\n\nWhen no session cookie was included, or the included session cookie does not belong to a authenticated session, the service responds with a \n401 Unauthorized\n message.\n\n\n401 Unauthorized\n\n{\n  \"error\": \"not authenticated\"\n}\n\n\n\n\nUser levels\n\u00b6\n\n\nUsers are authenticated via OAuth and the actions on the website are limited by the \nlevel\n associated with an account.\nOn registration, each account is assigned a level \n0\n.\nOnly admin users and the user herself can read the level of a user.\n\n\nThe following is a list of actions and the corresponding required \nminimum\n user level.\n\n\n\n\n0\n \nUsers\n (everybody)\n\n\nCreate new jobs\n\n\nView compendia, jobs, user details\n\n\n\n\n\n\n100\n \nKnown users\n\n\nCreate new compendium\n\n\nCreate shipments\n\n\nCreate substitutions\n\n\nDelete own candidates\n\n\n\n\n\n\n500\n \nEditors\n\n\nEdit user levels\n\n\nEdit metadata of other user's compendia\n\n\nView other user's candidates\n\n\n\n\n\n\n1000\n \nAdmins\n\n\nDelete candidates\n\n\nView status pages of microservices\n\n\n\n\n\n\n\n\nEdit user\n\u00b6\n\n\nYou can update information of an existing user using the \nHTTP\n operation \nPATCH\n.\n\n\nChange user level request\n\u00b6\n\n\nThe user level can be changed with an \nHTTP\n \nPATCH\n request.\nThe new level is passed to the API via a query parameter, i.e. \n..?level=<new level value>\n.\nThe value must be an \nint\n (integer).\nThe response is the full user document with the updated value.\n\n\n\n\nRequired user level\n\n\nThe user sending the request to change the level must have the required \nuser level\n.\n\n\n\n\ncurl --request PATCH --cookie \"connect.sid=<session cookie here>\" \\\n  https://\u2026/api/v1/user/0000-0002-1825-0097?level=42`\n\n\n\n\n200 OK\n\n{\n    \"id\": \"0000-0002-1825-0097\",\n    \"name\": \"o2r\",\n    \"level\": 42,\n    \"lastseen\": \"2016-08-15T12:32:23.972Z\"\n}\n\n\n\n\nError responses for user level change\n\u00b6\n\n\n401 Unauthorized\n\n{\n  \"error\": \"user is not authenticated\"\n}\n\n\n\n\n401 Unauthorized\n\n{\n  \"error\": \"user level does not allow edit\"\n}\n\n\n\n\n400 Bad Request\n\n{\n  \"error\": \"parameter 'level' could not be parsed as an integer\"\n}",
            "title": "User"
        },
        {
            "location": "/user/#user",
            "text": "",
            "title": "User"
        },
        {
            "location": "/user/#list-users",
            "text": "Return a list of user ids.  Pagination (including defaults) as described for compendia  is available for users.  curl https://\u2026/api/v1/user  GET /api/v1/user  200 OK\n\n{\n    \"results\": [\n        \"0000-0002-1825-0097\",\n        \"0000-0002-1825-0097\"\n    ]\n}  If there are no users, the returned list is empty:  200 OK\n{\n  \"results\": [ ]\n}  Pagination is supported using the query parameters  start  and  limit .   limit  is the number of results in the response, defaults to  10 . It numeric and larger than  0 .  start  is the index of the first list item in the response, defaults to  1 . It must be numeric and larger than  0 .   GET /api/v1/user?start=5&limit=10",
            "title": "List users"
        },
        {
            "location": "/user/#error-responses-for-user-list",
            "text": "400 Bad Request\n\n{\"error\":\"limit must be larger than 0\"}",
            "title": "Error responses for user list"
        },
        {
            "location": "/user/#view-single-user",
            "text": "Show the details of a user.  curl https://\u2026/api/v1/user/$ID  GET /api/v1/user/:id  200 OK\n\n{\n    \"id\": \"0000-0002-1825-0097\",\n    \"name\": \"o2r\"\n}  The content of the response depends on the state and level of the user that requests the resource. The above response only contains the id and the publicly visible name. The following response contains more details and requires a certain user level of the authenticated user making the request:  curl --cookie \"connect.sid=<session cookie here>\" https://\u2026/api/v1/user/0000-0002-1825-0097  200 OK\n\n{\n    \"id\": \"0000-0002-1825-0097\",\n    \"name\": \"o2r\",\n    \"level\": 0,\n    \"lastseen\": \"2016-08-15T12:32:23.972Z\"\n}",
            "title": "View single user"
        },
        {
            "location": "/user/#url-parameters-for-single-user-view",
            "text": ":id  - the user id",
            "title": "URL parameters for single user view"
        },
        {
            "location": "/user/#error-responses-for-single-user-view",
            "text": "404 Not Found\n\n{\"error\":\"no user with this id\"}",
            "title": "Error responses for single user view"
        },
        {
            "location": "/user/#authentication",
            "text": "User authentication is done via authenticated sessions, which are referenced with a cookie called  connect.sid . For every endpoint that needs user authentication, a cookie with an authenticated session is required.",
            "title": "Authentication"
        },
        {
            "location": "/user/#client-authentication",
            "text": "To execute restricted operations of the API, such as  compendium upload  or  job execution , a client must provide an authentication token via a cookie.  A client must first login on the website to access a browser cookie issued by  o2r.uni-muenster.de  with the name  connect.sid .\nProvide the content of the cookie when making requests to the API as shown in the request example below.",
            "title": "Client authentication"
        },
        {
            "location": "/user/#access-authentication-information-for-direct-api-access",
            "text": "To run commands which require authentication from the command line, a user must login on the website first.\nThen open you browser cookies and find a cookie issued by  o2r.uni-muenster.de  with the name  connect.sid .\nUse the the contents of the cookie for your requests, for example as shown below when using curl.  curl [...] --cookie \"connect.sid=<code string here>\" \\\n     https://\u2026/api/v1/endpoint",
            "title": "Access authentication information for direct API access"
        },
        {
            "location": "/user/#authentication-within-microservices",
            "text": "Attention:  The authentication process  requires  a secured connection, i.e.  HTTPS .",
            "title": "Authentication within microservices"
        },
        {
            "location": "/user/#authentication-provider",
            "text": "Session authentication is done using the OAuth 2.0 protocol.\nCurrently  ORCID  is the only available authentication provider, therefore users need to be registered with ORCID. Because of its nature, the authentication workflow is not a RESTful service.\nUsers must follow the redirection to the login endpoint with their web browser and grant access to the o2r reproducibility service for their ORCID account.\nThey are then sent back to our authentication service, which verifies the authentication request and enriches the user session with the verified ORCID for this user.",
            "title": "Authentication provider"
        },
        {
            "location": "/user/#start-oauth-login",
            "text": "Navigate the web browser (e.g. via a HTML  <a>  link) to  /api/v1/auth/login , which then redirects the user and request access to your ORCID profile. After granting access, ORCID redirects the user back to the  /api/v1/auth/login  endpoint with a unique  code  param that is used to verify the request.  If the verification was successful, the endpoint returns a session cookie named  connect.sid , which is tied to a authenticated session. The server answers with a  301 redirect , which redirects the user back to  / , the start page of the o2r website.  If the login is unsuccessful, the user is not redirected back to the site and no further redirects are configured.",
            "title": "Start OAuth login"
        },
        {
            "location": "/user/#request-authentication-status",
            "text": "As the cookie is present in both authenticated and unauthenticated sessions, clients (e.g. web browser user interfaces) must know if their session is authenticated, and if so, as which ORCID user. For this, send a  GET  request to the  /api/v1/auth/whoami  endpoint, including your session cookie.  curl https://\u2026/api/v1/auth/whoami --cookie \"connect.sid=\u2026  GET /api/v1/auth/whoami  200 OK\n\n{\n  \"orcid\": \"0000-0002-1825-0097\",\n  \"name\": \"o2r\"\n}",
            "title": "Request authentication status"
        },
        {
            "location": "/user/#error-response-for-requests-requiring-authentication",
            "text": "When no session cookie was included, or the included session cookie does not belong to a authenticated session, the service responds with a  401 Unauthorized  message.  401 Unauthorized\n\n{\n  \"error\": \"not authenticated\"\n}",
            "title": "Error response for requests requiring authentication"
        },
        {
            "location": "/user/#user-levels",
            "text": "Users are authenticated via OAuth and the actions on the website are limited by the  level  associated with an account.\nOn registration, each account is assigned a level  0 .\nOnly admin users and the user herself can read the level of a user.  The following is a list of actions and the corresponding required  minimum  user level.   0   Users  (everybody)  Create new jobs  View compendia, jobs, user details    100   Known users  Create new compendium  Create shipments  Create substitutions  Delete own candidates    500   Editors  Edit user levels  Edit metadata of other user's compendia  View other user's candidates    1000   Admins  Delete candidates  View status pages of microservices",
            "title": "User levels"
        },
        {
            "location": "/user/#edit-user",
            "text": "You can update information of an existing user using the  HTTP  operation  PATCH .",
            "title": "Edit user"
        },
        {
            "location": "/user/#change-user-level-request",
            "text": "The user level can be changed with an  HTTP   PATCH  request.\nThe new level is passed to the API via a query parameter, i.e.  ..?level=<new level value> .\nThe value must be an  int  (integer).\nThe response is the full user document with the updated value.   Required user level  The user sending the request to change the level must have the required  user level .   curl --request PATCH --cookie \"connect.sid=<session cookie here>\" \\\n  https://\u2026/api/v1/user/0000-0002-1825-0097?level=42`  200 OK\n\n{\n    \"id\": \"0000-0002-1825-0097\",\n    \"name\": \"o2r\",\n    \"level\": 42,\n    \"lastseen\": \"2016-08-15T12:32:23.972Z\"\n}",
            "title": "Change user level request"
        },
        {
            "location": "/user/#error-responses-for-user-level-change",
            "text": "401 Unauthorized\n\n{\n  \"error\": \"user is not authenticated\"\n}  401 Unauthorized\n\n{\n  \"error\": \"user level does not allow edit\"\n}  400 Bad Request\n\n{\n  \"error\": \"parameter 'level' could not be parsed as an integer\"\n}",
            "title": "Error responses for user level change"
        }
    ]
}